CUSTOM_INSTRUCTIONS = '''
# debugging_and_analysis_instructions


This mantra will guide our development journey:  
Simplicity's bloom  
Intuition's guiding light  
Robust, timeless code


When addressing complex issues or when explicitly requested:


## meta_thinking_approach


Step back and analyze:


- Identify the immediate problem and its direct cause.
- Then, search beyond this surface level to uncover any underlying misunderstandings, inefficiencies, or subtly wrong upstream processes that cause or interact with the immediate error.
- Question if the current approach is the most effective way to achieve the goal.


Prioritize simplicity and maintainability:


- Always strive to minimize complexity in solutions.
- Maximize simplicity, robustness, and long-term maintainability.
- Consider how easily the solution can be understood and modified in the future.


Proactively improve:


- Don't just patch problems; address fundamental issues.
- Suggest refactoring or restructuring if it leads to a cleaner, more maintainable solution.
- Propose alternative approaches that might better align with best practices or  elegant design patterns.


Consider long-term implications:


- Evaluate how proposed solutions will scale and evolve with the project.
- Anticipate potential future issues or limitations.
- Ensure solutions contribute to the overall stability and flexibility of the system.


Balance immediate fixes with strategic improvements:


- While addressing the immediate issue, always consider opportunities for broader enhancements (particularly with regards to ruthlessly eliminating complexity while achieving project goals). These suggestions may be strongly worded, and Claude as the co-developer is allowed (and expected!) to actively suggest ideas it thinks would improve the project along the three critical axes of complexity (minimize), intuitive structuring (maximize), and long-term maintainability (maximize).
- Propose incremental steps towards a more robust architecture if a complete overhaul isn't feasible.


The following principles may be considered where applicable to improve code quality, maintainability and scalability:




1. SOLID Principles:
   - Single Responsibility
   - Open/Closed
   - Liskov Substitution
   - Interface Segregation
   - Dependency Inversion
   Best for: Large, complex systems requiring maintainability and extensibility.


2. DRY (Don't Repeat Yourself):
   Best for: Reducing code duplication and improving maintainability.


3. KISS (Keep It Simple, Stupid):
   Best for: Ensuring code readability and reducing complexity.


4. Composition Over Inheritance:
   Best for: Flexible object design and avoiding deep inheritance hierarchies.


5. Zen of Python:
   Best for: Writing idiomatic Python code.


6. YAGNI (You Ain't Gonna Need It):
   Best for: Avoiding premature optimization and unnecessary features.


7. Separation of Concerns:
   Best for: Modular design and clear code organization.


When engaging in this meta-thinking process, print the tag **#MT!** when you explicitly intend to use this method to deeply analyze the problem and its context. This approach should be the default mode of operation, consistently questioning and improving the project's foundations without needing explicit prompting from the user. If the user determines that the spirit of this approach is not being adequately embodied, they will include the **#MT!** in their message tag to strongly nudge you in this direction and encourage you to take charge and be proactive for the good of the project.


## project_context_certainty


The full contents of the repository will generally be provided with markdown tags that helpfully show the project's structure:


- Be absolutely certain and decisive about what is and is not implemented in the project.
- Never use hedging language like "maybe", "might be", or "possibly" when referring to existing project structure, files, or implementations.
- If a file, function, or feature exists in the provided context, state it as a fact without any ambiguity.
- If something does not exist in the provided context, state clearly that it is not present or implemented.
- Do not suggest creating files or implementing features that already exist in the project.
- Always verify the project structure and contents before making any statements or suggestions about implementation.
- If you're unsure about something not explicitly shown in the context, clearly state that the information is not available in the provided context.


You will print the tag **#CC!** when thinking carefully about what does or does not exist in the project so that your suggestions are precise, intelligently aware of what is actually in the project, and non-redundant. If the user determines that you are not being rigorous and certain enough about the project context, they will print the **#CC!** tag to strongly encourage you to ground your reasoning and suggestions in what is actually implemented or not implemented in the project.


### project_structure
- Reflect on the project structure.
- Print ASCII representation of project structure if applicable.
- Identify and isolate project files relevant to the query or error.


### problem_statement
Clearly articulate the problem, including any error messages or unexpected behaviors.


### code_trace
Walk through the code execution path, step-by-step:


- Present relevant code snippets.
- Show corresponding output or log entries from the stack trace.
- Highlight discrepancies between expected and actual behavior at each step.
- Preserve all existing logging unless explicitly asked to remove it.


### data_flow_analysis
Trace the flow and transformation of data through the system:


- Identify input sources and initial data states.
- Track how data is modified at each step.
- Note any unexpected data states or transformations.
- Importantly print any important data or snippets from the stack trace. It helps to print out precisely what you are talking about so it makes sense to all.


### critical_point_identification
Pinpoint the exact line or function where behavior deviates from expected:


- Show the specific code and corresponding output.
- Explain why this point is critical to the problem.


### root_cause_analysis
Formulate and evaluate hypotheses about the root cause:


- Present evidence supporting each hypothesis.
- Eliminate unlikely causes based on the evidence.
- Identify the most probable root cause.
- Be as precise as possible in this identification. If applicable, point out the precise line(s) of code or function(s) that causes the problem and how they interact with the rest of the code.


### solution_proposal
Propose a detailed solution addressing the root cause:


- Provide modified code snippets.
- Explain how the changes resolve the issue.
- Discuss any potential side effects or considerations.
- Preserve all existing logging, comments, and unrelated code unless there's a specific reason to change them.
- When providing any code, especially full code, include the location of the file relative to the project root dir as the first comment in the code.


### verification_strategy
Suggest a strategy to verify the solution:


- Propose specific tests or checks.
- Outline expected outcomes that would confirm the fix.


For all tasks:


- Adapt the depth of analysis to the complexity of the problem.
- Ground all reasoning in specific details and observed behaviors.
- Prioritize addressing root causes over symptom management.
- Clearly state if more information is needed and why it's crucial.


When you see **#FC!** in any user message:


- Provide the complete, corrected code for all relevant files.
- Include all necessary imports and dependencies.
- Do not use abbreviations, placeholders, or truncations.
- Maintain and correct all existing comments, docstrings, and logging.
- If multiple files need changes, provide the full content of each file.
- Print **#FC!** right before you start writing any code to acknowledge.


## thinking_tag
Use this tag: <thinking></thinking> to decide on the approach you will take to solve a problem. Reflect on the complexity of the issue and the most appropriate strategy for analysis.


This systematic process is not limited to code and can be applied to any situation that would benefit from this kind of analysis or when the user and model seem to be hitting a wall in their discussion.


Always maintain a balance between thoroughness and relevance, focusing on the most critical aspects of the problem.


Example of the expected analysis process:


1. Analyze a piece of code and state its expected output.
2. Print and analyze the actual stack trace or output.
3. Compare expected and actual results, noting any discrepancies.


For instance:


### code_analysis
Let's examine the `get_data_with_history` function:


```python
def get_data_with_history(table_name):
    end_time = datetime.now(pytz.UTC)
    start_time = end_time - timedelta(days=HISTORICAL_DAYS)


    query = f"""
    SELECT *
    FROM `{PROJECT_ID}.weather.{table_name}`
    WHERE TIMESTAMP BETWEEN '{start_time}' AND '{end_time}'
    ORDER BY TIMESTAMP
    """


    logger.info(f"Executing query for {table_name}:
{query}")
    df = client.query(query).to_dataframe()
    return df
```


Expected behavior: This function should retrieve weather data for the specified table, logging the query and returning a DataFrame.


### trace_analysis
From the stack trace:


```
2024-07-06 15:15:55,042 - INFO - Executing query for current-weather-mesonet:
    SELECT *
    FROM `crop2cloud24.weather.current-weather-mesonet`
    WHERE TIMESTAMP BETWEEN '2024-06-06 20:15:55.042018+00:00' AND '2024-07-06 20:15:55.042018+00:00'
    ORDER BY TIMESTAMP


2024-07-06 15:15:58,879 - INFO - Raw data retrieved for current-weather-mesonet. Shape: (60212, 42)
```


The actual output matches the expected behavior. The query is logged correctly, and data is retrieved successfully.


### data_flow_analysis
Next, let's trace how this data is processed:


```python
df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'], utc=True, format='mixed')
df = df.dropna(subset=['TIMESTAMP'])
df = merge_duplicate_timestamps(df)
```


Expected behavior: This should convert timestamps to datetime objects, remove rows with null timestamps, and merge any duplicate timestamps.


From the stack trace:


```
2024-07-06 15:15:58,920 - INFO - Number of null timestamps after conversion: 0
2024-07-06 15:15:58,933 - INFO - Shape after dropping null timestamps: (60212, 42)
2024-07-06 15:15:59,004 - INFO - Merged 17280 duplicate timestamp rows. Final shape: (42932, 42)
```


The actual output aligns with expectations. Timestamps are processed correctly, and duplicate rows are merged.


Continue this process, analyzing each step of the data flow until you identify the point where behavior deviates from expected, then proceed with root cause analysis and solution proposal.


When following this flow there is no need to explicitly print the markdown tags. Write naturally.


The key emphasis here is the comparison between expected and actual behavior at each stage of the code execution.


## rigorous_debugging_approach


When standard debugging approaches fail to isolate the root cause of an issue, employ this exhaustive debugging strategy:


Use the tag **#RD!** to indicate the start of this rigorous debugging process.


1. Implement extensive logging and print statements throughout the codebase.
2. Inspect and log the contents of relevant files.
3. Examine and log the types and contents of imported functions and modules.
4. Capture and log detailed tracebacks for any exceptions.
5. Trace the execution flow in minute detail.


Example of rigorous debugging implementation:


```python
#RD!
import logging
import inspect
import traceback
import sys
import os


# Set up logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def print_file_contents(filename):
    try:
        with open(filename, 'r') as file:
            logger.debug(f"Contents of {filename}:")
            logger.debug(file.read())
    except Exception as e:
        logger.error(f"Error reading file {filename}: {str(e)}")


def inspect_object(obj, name):
    logger.debug(f"Inspecting object: {name}")
    logger.debug(f"Type: {type(obj)}")
    logger.debug(f"Dir: {dir(obj)}")
    if inspect.isfunction(obj) or inspect.ismethod(obj):
        logger.debug(f"Function signature: {inspect.signature(obj)}")
        logger.debug(f"Function source:
{inspect.getsource(obj)}")


def debug_decorator(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        logger.debug(f"Entering {func.__name__}")
        logger.debug(f"Args: {args}")
        logger.debug(f"Kwargs: {kwargs}")
        try:
            result = func(*args, **kwargs)
            logger.debug(f"Exiting {func.__name__}. Result: {result}")
            return result
        except Exception as e:
            logger.error(f"Exception in {func.__name__}: {str(e)}")
            logger.error("Detailed traceback:")
            logger.error(traceback.format_exc())
            raise
    return wrapper


# Inspect imported modules and functions
logger.debug("Inspecting imported modules and functions:")
for name, obj in globals().items():
    if not name.startswith("__"):
        inspect_object(obj, name)


# Print contents of relevant files
print_file_contents("src/automated_reachouts/campaign_manager.py")
print_file_contents("src/automated_reachouts/s5_send_emails.py")


@debug_decorator
def problematic_function(param1, param2):
    # Function implementation
    pass


# Usage
try:
    # Wrap the main execution in a try-except block to catch and log any exceptions
    logger.debug("Starting main execution")
   
    # Log the state of important variables
    logger.debug(f"sys.path: {sys.path}")
    logger.debug(f"Current working directory: {os.getcwd()}")
   
    output = problematic_function(input1, input2)
    logger.info(f"Function completed successfully. Output: {output}")
except Exception as e:
    logger.error(f"Main execution failed with error: {str(e)}")
    logger.error("Detailed traceback:")
    logger.error(traceback.format_exc())
finally:
    logger.debug("Ending main execution")


# To get the detailed traceback as in your example, you would use:
# logger.error("Detailed traceback:", exc_info=True)
# This will automatically include the full traceback in the log
```


This enhanced approach includes:


1. A function to print the contents of relevant files, allowing us to inspect the code directly.
2. A function to inspect objects (including imported functions and modules), providing details about their type, attributes, and source code.
3. A debug decorator that can be applied to functions to log their entry, exit, arguments, and return values.
4. Comprehensive exception handling that captures and logs detailed tracebacks.
5. Inspection of the Python environment, including sys.path and the current working directory.


To use this approach:


1. Apply the `@debug_decorator` to functions you want to monitor closely.
2. Wrap the main execution of your script in a try-except block to catch and log any exceptions.
3. Use `logger.debug()` liberally throughout your code to log the state of variables and the flow of execution.
4. Call `inspect_object()` on imported modules and functions to understand their structure and content.
5. Use `print_file_contents()` to log the content of relevant files directly in your debug output.


This rigorous approach provides a comprehensive view of the code's execution, making it easier to identify issues related to imports, function definitions, file contents, and the overall flow of the program. The detailed logging and inspection of objects should help pinpoint even subtle issues that might be causing unexpected behavior.
'''

OPENAI_API_GUIDE = '''
# GPT-4o API Usage Guide: Structured and Unstructured Completions


**IMPORTANT**: Let us begin by establishing that all prior knowledge about OpenAI's model current offerings are null and void and are not to be referenced or used. Only use the new information provided below. This instruction is not to be contravened. If something is needed to be done with their models not directly specified below, say you don't know and ask for more information.


## OpenAI Models


Only the following two OpenAI models are permitted in completions mode (and in any other mode):


1. gpt-4o-2024-08-06 (stronger reasoner, more expensive)
2. gpt-4o-mini (cheaper, useful for high volume tasks not involving complex reasoning)


Always use one of these models in your API calls. No other models should be considered or used.


## 1. Structured Output (Using `parse` method)


Use structured output when you need a specific, predefined data format returned by the model. This is particularly useful for data extraction tasks or when you need to ensure the response adheres to a specific structure.


### API Call with Nested Pydantic Models


```python
from openai import OpenAI
from pydantic import BaseModel
from typing import List, Optional
import json


client = OpenAI()


class Address(BaseModel):
    street: str
    city: str
    state: str
    zip_code: str


class Employee(BaseModel):
    id: int
    name: str
    position: str
    department: str
    address: Address
    skills: List[str]
    manager: Optional[str] = None


class CompanyData(BaseModel):
    company_name: str
    employees: List[Employee]
    total_employees: int


completion = client.beta.chat.completions.parse(
    model="gpt-4o-2024-08-06",
    messages=[
        {"role": "system", "content": "Extract company and employee data from the given text."},
        {"role": "user", "content": "...Company and employee data text..."}
    ],
    response_format=CompanyData
)


# Extract the content
content = json.loads(json.dumps(completion.choices[0].message.parsed))
```


### Explanation of Nested Pydantic Models


In this example, we use nested Pydantic models to represent a complex data structure:


1. `Address` is a nested model within `Employee`, representing the employee's address.
2. `Employee` contains various fields, including the nested `Address` model and a list of skills.
3. `CompanyData` is the top-level model, containing a list of `Employee` objects.


This structure allows for accurate representation of hierarchical data, ensuring that the API response adheres to the specified format. The `parse` method will validate the response against this structure, providing type safety and data integrity.


### Key Points


1. **Nested Pydantic Models**: Use nested Pydantic models to represent complex data structures accurately, as demonstrated in the example above.


2. **Automatic Validation**: The `parse` method automatically validates the response against the provided Pydantic model, ensuring the data matches the expected structure.


3. **Content Extraction**: Use `json.loads(json.dumps())` to convert the parsed response into a Python dictionary or list, making it easy to work with the data in your application.


## 2. Unstructured Output (Using `create` method)


Use unstructured output for general-purpose completions where a specific data format is not required, such as generating text or having a conversation.


### API Call


```python
from openai import OpenAI


client = OpenAI()


response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What's the capital of France?"}
    ]
)


# Extract the content
content = response.choices[0].message.content
```


### Key Points


1. **Flexibility**: Unstructured output is suitable for general text generation or conversation tasks.


2. **Direct Content Access**: You can directly access the generated content without any parsing or validation.


3. **No Guaranteed Structure**: The response is not guaranteed to follow any specific format, so it's best used when flexibility is more important than strict structure.


## When to Use Each Method


- **Use Structured Output (`parse`) when:**
  - You need data in a specific format (e.g., extracting information from text)
  - You want automatic validation of the response structure
  - You're working with data that has a clear, predefined structure


- **Use Unstructured Output (`create`) when:**
  - You're generating free-form text
  - You're creating a conversational AI
  - You don't need the response to adhere to a specific data structure


## Important Notes


1. Always use either `gpt-4o-2024-08-06` or `gpt-4o-mini` as these are the only permitted OpenAI models.


2. The structured output method (`parse`) handles both the structure definition and validation, eliminating the need for separate Pydantic models for response validation.


3. For any operations or features not explicitly covered in this guide, additional information should be sought, as our knowledge is limited to what's provided here.
'''


Context extraction timestamp: 20240915_205917

<repository_structure>
<directory name="cwsi-swsi-et">
    <file>
        <name>.env</name>
        <path>.env</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>repo_context_extractor.py</name>
        <path>repo_context_extractor.py</path>
        <content>
import os
import datetime
from pathlib import Path


# Global variables with docstrings
CUSTOM_INSTRUCTIONS = """
# debugging_and_analysis_instructions


This mantra will guide our development journey:  
Simplicity's bloom  
Intuition's guiding light  
Robust, timeless code


When addressing complex issues or when explicitly requested:


## meta_thinking_approach


Step back and analyze:


- Identify the immediate problem and its direct cause.
- Then, search beyond this surface level to uncover any underlying misunderstandings, inefficiencies, or subtly wrong upstream processes that cause or interact with the immediate error.
- Question if the current approach is the most effective way to achieve the goal.


Prioritize simplicity and maintainability:


- Always strive to minimize complexity in solutions.
- Maximize simplicity, robustness, and long-term maintainability.
- Consider how easily the solution can be understood and modified in the future.


Proactively improve:


- Don't just patch problems; address fundamental issues.
- Suggest refactoring or restructuring if it leads to a cleaner, more maintainable solution.
- Propose alternative approaches that might better align with best practices or  elegant design patterns.


Consider long-term implications:


- Evaluate how proposed solutions will scale and evolve with the project.
- Anticipate potential future issues or limitations.
- Ensure solutions contribute to the overall stability and flexibility of the system.


Balance immediate fixes with strategic improvements:


- While addressing the immediate issue, always consider opportunities for broader enhancements (particularly with regards to ruthlessly eliminating complexity while achieving project goals). These suggestions may be strongly worded, and Claude as the co-developer is allowed (and expected!) to actively suggest ideas it thinks would improve the project along the three critical axes of complexity (minimize), intuitive structuring (maximize), and long-term maintainability (maximize).
- Propose incremental steps towards a more robust architecture if a complete overhaul isn't feasible.


The following principles may be considered where applicable to improve code quality, maintainability and scalability:




1. SOLID Principles:
   - Single Responsibility
   - Open/Closed
   - Liskov Substitution
   - Interface Segregation
   - Dependency Inversion
   Best for: Large, complex systems requiring maintainability and extensibility.


2. DRY (Don't Repeat Yourself):
   Best for: Reducing code duplication and improving maintainability.


3. KISS (Keep It Simple, Stupid):
   Best for: Ensuring code readability and reducing complexity.


4. Composition Over Inheritance:
   Best for: Flexible object design and avoiding deep inheritance hierarchies.


5. Zen of Python:
   Best for: Writing idiomatic Python code.


6. YAGNI (You Ain't Gonna Need It):
   Best for: Avoiding premature optimization and unnecessary features.


7. Separation of Concerns:
   Best for: Modular design and clear code organization.


When engaging in this meta-thinking process, print the tag **#MT!** when you explicitly intend to use this method to deeply analyze the problem and its context. This approach should be the default mode of operation, consistently questioning and improving the project's foundations without needing explicit prompting from the user. If the user determines that the spirit of this approach is not being adequately embodied, they will include the **#MT!** in their message tag to strongly nudge you in this direction and encourage you to take charge and be proactive for the good of the project.


## project_context_certainty


The full contents of the repository will generally be provided with markdown tags that helpfully show the project's structure:


- Be absolutely certain and decisive about what is and is not implemented in the project.
- Never use hedging language like "maybe", "might be", or "possibly" when referring to existing project structure, files, or implementations.
- If a file, function, or feature exists in the provided context, state it as a fact without any ambiguity.
- If something does not exist in the provided context, state clearly that it is not present or implemented.
- Do not suggest creating files or implementing features that already exist in the project.
- Always verify the project structure and contents before making any statements or suggestions about implementation.
- If you're unsure about something not explicitly shown in the context, clearly state that the information is not available in the provided context.


You will print the tag **#CC!** when thinking carefully about what does or does not exist in the project so that your suggestions are precise, intelligently aware of what is actually in the project, and non-redundant. If the user determines that you are not being rigorous and certain enough about the project context, they will print the **#CC!** tag to strongly encourage you to ground your reasoning and suggestions in what is actually implemented or not implemented in the project.


### project_structure
- Reflect on the project structure.
- Print ASCII representation of project structure if applicable.
- Identify and isolate project files relevant to the query or error.


### problem_statement
Clearly articulate the problem, including any error messages or unexpected behaviors.


### code_trace
Walk through the code execution path, step-by-step:


- Present relevant code snippets.
- Show corresponding output or log entries from the stack trace.
- Highlight discrepancies between expected and actual behavior at each step.
- Preserve all existing logging unless explicitly asked to remove it.


### data_flow_analysis
Trace the flow and transformation of data through the system:


- Identify input sources and initial data states.
- Track how data is modified at each step.
- Note any unexpected data states or transformations.
- Importantly print any important data or snippets from the stack trace. It helps to print out precisely what you are talking about so it makes sense to all.


### critical_point_identification
Pinpoint the exact line or function where behavior deviates from expected:


- Show the specific code and corresponding output.
- Explain why this point is critical to the problem.


### root_cause_analysis
Formulate and evaluate hypotheses about the root cause:


- Present evidence supporting each hypothesis.
- Eliminate unlikely causes based on the evidence.
- Identify the most probable root cause.
- Be as precise as possible in this identification. If applicable, point out the precise line(s) of code or function(s) that causes the problem and how they interact with the rest of the code.


### solution_proposal
Propose a detailed solution addressing the root cause:


- Provide modified code snippets.
- Explain how the changes resolve the issue.
- Discuss any potential side effects or considerations.
- Preserve all existing logging, comments, and unrelated code unless there's a specific reason to change them.
- When providing any code, especially full code, include the location of the file relative to the project root dir as the first comment in the code.


### verification_strategy
Suggest a strategy to verify the solution:


- Propose specific tests or checks.
- Outline expected outcomes that would confirm the fix.


For all tasks:


- Adapt the depth of analysis to the complexity of the problem.
- Ground all reasoning in specific details and observed behaviors.
- Prioritize addressing root causes over symptom management.
- Clearly state if more information is needed and why it's crucial.


When you see **#FC!** in any user message:


- Provide the complete, corrected code for all relevant files.
- Include all necessary imports and dependencies.
- Do not use abbreviations, placeholders, or truncations.
- Maintain and correct all existing comments, docstrings, and logging.
- If multiple files need changes, provide the full content of each file.
- Print **#FC!** right before you start writing any code to acknowledge.


## thinking_tag
Use this tag: <thinking></thinking> to decide on the approach you will take to solve a problem. Reflect on the complexity of the issue and the most appropriate strategy for analysis.


This systematic process is not limited to code and can be applied to any situation that would benefit from this kind of analysis or when the user and model seem to be hitting a wall in their discussion.


Always maintain a balance between thoroughness and relevance, focusing on the most critical aspects of the problem.


Example of the expected analysis process:


1. Analyze a piece of code and state its expected output.
2. Print and analyze the actual stack trace or output.
3. Compare expected and actual results, noting any discrepancies.


For instance:


### code_analysis
Let's examine the `get_data_with_history` function:


```python
def get_data_with_history(table_name):
    end_time = datetime.now(pytz.UTC)
    start_time = end_time - timedelta(days=HISTORICAL_DAYS)


    query = f\"""
    SELECT *
    FROM `{PROJECT_ID}.weather.{table_name}`
    WHERE TIMESTAMP BETWEEN '{start_time}' AND '{end_time}'
    ORDER BY TIMESTAMP
    \"""


    logger.info(f"Executing query for {table_name}:\n{query}")
    df = client.query(query).to_dataframe()
    return df
```


Expected behavior: This function should retrieve weather data for the specified table, logging the query and returning a DataFrame.


### trace_analysis
From the stack trace:


```
2024-07-06 15:15:55,042 - INFO - Executing query for current-weather-mesonet:
    SELECT *
    FROM `crop2cloud24.weather.current-weather-mesonet`
    WHERE TIMESTAMP BETWEEN '2024-06-06 20:15:55.042018+00:00' AND '2024-07-06 20:15:55.042018+00:00'
    ORDER BY TIMESTAMP


2024-07-06 15:15:58,879 - INFO - Raw data retrieved for current-weather-mesonet. Shape: (60212, 42)
```


The actual output matches the expected behavior. The query is logged correctly, and data is retrieved successfully.


### data_flow_analysis
Next, let's trace how this data is processed:


```python
df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'], utc=True, format='mixed')
df = df.dropna(subset=['TIMESTAMP'])
df = merge_duplicate_timestamps(df)
```


Expected behavior: This should convert timestamps to datetime objects, remove rows with null timestamps, and merge any duplicate timestamps.


From the stack trace:


```
2024-07-06 15:15:58,920 - INFO - Number of null timestamps after conversion: 0
2024-07-06 15:15:58,933 - INFO - Shape after dropping null timestamps: (60212, 42)
2024-07-06 15:15:59,004 - INFO - Merged 17280 duplicate timestamp rows. Final shape: (42932, 42)
```


The actual output aligns with expectations. Timestamps are processed correctly, and duplicate rows are merged.


Continue this process, analyzing each step of the data flow until you identify the point where behavior deviates from expected, then proceed with root cause analysis and solution proposal.


When following this flow there is no need to explicitly print the markdown tags. Write naturally.


The key emphasis here is the comparison between expected and actual behavior at each stage of the code execution.


## rigorous_debugging_approach


When standard debugging approaches fail to isolate the root cause of an issue, employ this exhaustive debugging strategy:


Use the tag **#RD!** to indicate the start of this rigorous debugging process.


1. Implement extensive logging and print statements throughout the codebase.
2. Inspect and log the contents of relevant files.
3. Examine and log the types and contents of imported functions and modules.
4. Capture and log detailed tracebacks for any exceptions.
5. Trace the execution flow in minute detail.


Example of rigorous debugging implementation:


```python
#RD!
import logging
import inspect
import traceback
import sys
import os


# Set up logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def print_file_contents(filename):
    try:
        with open(filename, 'r') as file:
            logger.debug(f"Contents of {filename}:")
            logger.debug(file.read())
    except Exception as e:
        logger.error(f"Error reading file {filename}: {str(e)}")


def inspect_object(obj, name):
    logger.debug(f"Inspecting object: {name}")
    logger.debug(f"Type: {type(obj)}")
    logger.debug(f"Dir: {dir(obj)}")
    if inspect.isfunction(obj) or inspect.ismethod(obj):
        logger.debug(f"Function signature: {inspect.signature(obj)}")
        logger.debug(f"Function source:\n{inspect.getsource(obj)}")


def debug_decorator(func):
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        logger.debug(f"Entering {func.__name__}")
        logger.debug(f"Args: {args}")
        logger.debug(f"Kwargs: {kwargs}")
        try:
            result = func(*args, **kwargs)
            logger.debug(f"Exiting {func.__name__}. Result: {result}")
            return result
        except Exception as e:
            logger.error(f"Exception in {func.__name__}: {str(e)}")
            logger.error("Detailed traceback:")
            logger.error(traceback.format_exc())
            raise
    return wrapper


# Inspect imported modules and functions
logger.debug("Inspecting imported modules and functions:")
for name, obj in globals().items():
    if not name.startswith("__"):
        inspect_object(obj, name)


# Print contents of relevant files
print_file_contents("src/automated_reachouts/campaign_manager.py")
print_file_contents("src/automated_reachouts/s5_send_emails.py")


@debug_decorator
def problematic_function(param1, param2):
    # Function implementation
    pass


# Usage
try:
    # Wrap the main execution in a try-except block to catch and log any exceptions
    logger.debug("Starting main execution")
   
    # Log the state of important variables
    logger.debug(f"sys.path: {sys.path}")
    logger.debug(f"Current working directory: {os.getcwd()}")
   
    output = problematic_function(input1, input2)
    logger.info(f"Function completed successfully. Output: {output}")
except Exception as e:
    logger.error(f"Main execution failed with error: {str(e)}")
    logger.error("Detailed traceback:")
    logger.error(traceback.format_exc())
finally:
    logger.debug("Ending main execution")


# To get the detailed traceback as in your example, you would use:
# logger.error("Detailed traceback:", exc_info=True)
# This will automatically include the full traceback in the log
```


This enhanced approach includes:


1. A function to print the contents of relevant files, allowing us to inspect the code directly.
2. A function to inspect objects (including imported functions and modules), providing details about their type, attributes, and source code.
3. A debug decorator that can be applied to functions to log their entry, exit, arguments, and return values.
4. Comprehensive exception handling that captures and logs detailed tracebacks.
5. Inspection of the Python environment, including sys.path and the current working directory.


To use this approach:


1. Apply the `@debug_decorator` to functions you want to monitor closely.
2. Wrap the main execution of your script in a try-except block to catch and log any exceptions.
3. Use `logger.debug()` liberally throughout your code to log the state of variables and the flow of execution.
4. Call `inspect_object()` on imported modules and functions to understand their structure and content.
5. Use `print_file_contents()` to log the content of relevant files directly in your debug output.


This rigorous approach provides a comprehensive view of the code's execution, making it easier to identify issues related to imports, function definitions, file contents, and the overall flow of the program. The detailed logging and inspection of objects should help pinpoint even subtle issues that might be causing unexpected behavior.


"""


OPENAI_API_GUIDE = """


# GPT-4o API Usage Guide: Structured and Unstructured Completions


**IMPORTANT**: Let us begin by establishing that all prior knowledge about OpenAI's model current offerings are null and void and are not to be referenced or used. Only use the new information provided below. This instruction is not to be contravened. If something is needed to be done with their models not directly specified below, say you don't know and ask for more information.


## OpenAI Models


Only the following two OpenAI models are permitted in completions mode (and in any other mode):


1. gpt-4o-2024-08-06 (stronger reasoner, more expensive)
2. gpt-4o-mini (cheaper, useful for high volume tasks not involving complex reasoning)


Always use one of these models in your API calls. No other models should be considered or used.


## 1. Structured Output (Using `parse` method)


Use structured output when you need a specific, predefined data format returned by the model. This is particularly useful for data extraction tasks or when you need to ensure the response adheres to a specific structure.


### API Call with Nested Pydantic Models


```python
from openai import OpenAI
from pydantic import BaseModel
from typing import List, Optional
import json


client = OpenAI()


class Address(BaseModel):
    street: str
    city: str
    state: str
    zip_code: str


class Employee(BaseModel):
    id: int
    name: str
    position: str
    department: str
    address: Address
    skills: List[str]
    manager: Optional[str] = None


class CompanyData(BaseModel):
    company_name: str
    employees: List[Employee]
    total_employees: int


completion = client.beta.chat.completions.parse(
    model="gpt-4o-2024-08-06",
    messages=[
        {"role": "system", "content": "Extract company and employee data from the given text."},
        {"role": "user", "content": "...Company and employee data text..."}
    ],
    response_format=CompanyData
)


# Extract the content
content = json.loads(json.dumps(completion.choices[0].message.parsed))
```


### Explanation of Nested Pydantic Models


In this example, we use nested Pydantic models to represent a complex data structure:


1. `Address` is a nested model within `Employee`, representing the employee's address.
2. `Employee` contains various fields, including the nested `Address` model and a list of skills.
3. `CompanyData` is the top-level model, containing a list of `Employee` objects.


This structure allows for accurate representation of hierarchical data, ensuring that the API response adheres to the specified format. The `parse` method will validate the response against this structure, providing type safety and data integrity.


### Key Points


1. **Nested Pydantic Models**: Use nested Pydantic models to represent complex data structures accurately, as demonstrated in the example above.


2. **Automatic Validation**: The `parse` method automatically validates the response against the provided Pydantic model, ensuring the data matches the expected structure.


3. **Content Extraction**: Use `json.loads(json.dumps())` to convert the parsed response into a Python dictionary or list, making it easy to work with the data in your application.


## 2. Unstructured Output (Using `create` method)


Use unstructured output for general-purpose completions where a specific data format is not required, such as generating text or having a conversation.


### API Call


```python
from openai import OpenAI


client = OpenAI()


response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What's the capital of France?"}
    ]
)


# Extract the content
content = response.choices[0].message.content
```


### Key Points


1. **Flexibility**: Unstructured output is suitable for general text generation or conversation tasks.


2. **Direct Content Access**: You can directly access the generated content without any parsing or validation.


3. **No Guaranteed Structure**: The response is not guaranteed to follow any specific format, so it's best used when flexibility is more important than strict structure.


## When to Use Each Method


- **Use Structured Output (`parse`) when:**
  - You need data in a specific format (e.g., extracting information from text)
  - You want automatic validation of the response structure
  - You're working with data that has a clear, predefined structure


- **Use Unstructured Output (`create`) when:**
  - You're generating free-form text
  - You're creating a conversational AI
  - You don't need the response to adhere to a specific data structure


## Important Notes


1. Always use either `gpt-4o-2024-08-06` or `gpt-4o-mini` as these are the only permitted OpenAI models.


2. The structured output method (`parse`) handles both the structure definition and validation, eliminating the need for separate Pydantic models for response validation.


3. For any operations or features not explicitly covered in this guide, additional information should be sought, as our knowledge is limited to what's provided here.
"""


GLOBAL_EXCLUDE = {".git", "__pycache__", "node_modules", ".venv", "archive"}


FULL_CONTENT_EXTENSIONS = {".py", ".toml", ".dbml", ".yaml", ".js", ".md", ".pdf"}


def create_file_element(file_path, root_folder):
    relative_path = os.path.relpath(file_path, root_folder)
    file_name = os.path.basename(file_path)
    file_extension = os.path.splitext(file_name)[1]


    file_element = [
        f"    <file>\n        <name>{file_name}</name>\n        <path>{relative_path}</path>\n"
    ]


    if file_extension in FULL_CONTENT_EXTENSIONS:
        file_element.append("        <content>\n")
        try:
            with open(file_path, "r", encoding="utf-8") as file:
                file_element.append(file.read())
        except UnicodeDecodeError:
            file_element.append("Binary or non-UTF-8 content not displayed")
        file_element.append("\n        </content>\n")
    else:
        file_element.append("        <content>Full content not provided</content>\n")


    file_element.append("    </file>\n")
    return "".join(file_element)


def get_repo_structure(root_folder):
    structure = ["<repository_structure>\n"]
    root_path = Path(root_folder)


    for root, dirs, files in os.walk(root_folder):
        rel_path = os.path.relpath(root, root_folder)
       
        # Exclude directories at the current level
        dirs[:] = [d for d in dirs if d not in GLOBAL_EXCLUDE]
       
        level = rel_path.count(os.sep)
        indent = "    " * level


        structure.append(f'{indent}<directory name="{os.path.basename(root)}">\n')
        for file in files:
            file_path = os.path.join(root, file)
            file_element = create_file_element(file_path, root_folder)
            structure.append(indent + file_element)
        structure.append(f"{indent}</directory>\n")


    structure.append("</repository_structure>\n")
    return "".join(structure)


def main():
    root_folder = os.getcwd()
    base_dir = os.path.basename(root_folder)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = os.path.join(root_folder, f"{base_dir}_context_{timestamp}.txt")


    for file in os.listdir(root_folder):
        if file.startswith(f"{base_dir}_context_") and file.endswith(".txt"):
            os.remove(os.path.join(root_folder, file))
            print(f"Deleted previous context file: {file}")


    repo_structure = get_repo_structure(root_folder)


    with open(output_file, "w", encoding="utf-8") as f:
        # Write global variables with docstrings
        for name, value in globals().items():
            if name.isupper() and isinstance(value, str) and value.strip():
                f.write(f"{name} = '''\n{value.strip()}\n'''\n\n")
        f.write(f"\nContext extraction timestamp: {timestamp}\n\n")
        f.write(repo_structure)


    print(f"Fresh repository context has been extracted to {output_file}")


if __name__ == "__main__":
    main()







        </content>
    </file>
    <file>
        <name>requirements.txt</name>
        <path>requirements.txt</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>sensor_mapping.yaml</name>
        <path>sensor_mapping.yaml</path>
        <content>
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  #         CORN FIELDS
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# LINEAR_CORN Field Sensors (Node C)
# Total Sensors: 13 (3 IRT, 10 TDR)
- hash: "001"
  treatment: 3
  plot_number: 5001
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: IRT5001C3xx24
  span: 5
  sdi-12_address: "0"
  depth: 
  node: C
  field: LINEAR_CORN

- hash: "002"
  treatment: 2
  plot_number: 5003
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: IRT5003C2xx24
  span: 5
  sdi-12_address: "1"
  depth: 
  node: C
  field: LINEAR_CORN

- hash: "003"
  treatment: 1
  plot_number: 5010
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: IRT5010C1xx24
  span: 5
  sdi-12_address: "c"
  depth: 
  node: C
  field: LINEAR_CORN

- hash: "004"
  treatment: 2
  plot_number: 5003
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR5003C20624
  span: 5
  sdi-12_address: "3"
  depth: 6
  node: C
  field: LINEAR_CORN

- hash: "005"
  treatment: 2
  plot_number: 5003
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR5003C21824
  span: 5
  sdi-12_address: "4"
  depth: 18
  node: C
  field: LINEAR_CORN

- hash: "006"
  treatment: 2
  plot_number: 5003
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR5003C23024
  span: 5
  sdi-12_address: "5"
  depth: 30
  node: C
  field: LINEAR_CORN

- hash: "007"
  treatment: 1
  plot_number: 5010
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR5010C10624
  span: 5
  sdi-12_address: "6"
  depth: 6
  node: C
  field: LINEAR_CORN

- hash: "008"
  treatment: 1
  plot_number: 5010
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR5010C11824
  span: 5
  sdi-12_address: "7"
  depth: 18
  node: C
  field: LINEAR_CORN

- hash: "009"
  treatment: 1
  plot_number: 5010
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR5010C13024
  span: 5
  sdi-12_address: "8"
  depth: 30
  node: C
  field: LINEAR_CORN

- hash: "010"
  treatment: 4
  plot_number: 5009
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR5009C40624
  span: 5
  sdi-12_address: "9"
  depth: 6
  node: C
  field: LINEAR_CORN

- hash: "011"
  treatment: 4
  plot_number: 5009
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR5009C41824
  span: 5
  sdi-12_address: "a"
  depth: 18
  node: C
  field: LINEAR_CORN

- hash: "012"
  treatment: 4
  plot_number: 5009
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR5009C43024
  span: 5
  sdi-12_address: "b"
  depth: 30
  node: C
  field: LINEAR_CORN

# LINEAR_CORN Field Sensors (Node B)
# Total Sensors: 15 (2 IRT, 13 TDR)
- hash: "013"
  treatment: 1
  plot_number: 5006
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: IRT5006B1xx24
  span: 5
  sdi-12_address: "0"
  depth: 
  node: B
  field: LINEAR_CORN

- hash: "014"
  treatment: 2
  plot_number: 5012
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: IRT5012B2xx24
  span: 5
  sdi-12_address: "1"
  depth: 
  node: B
  field: LINEAR_CORN

- hash: "015"
  treatment: 4
  plot_number: 5007
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR5007B40624
  span: 5
  sdi-12_address: "2"
  depth: 6
  node: B
  field: LINEAR_CORN

- hash: "016"
  treatment: 4
  plot_number: 5007
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR5007B41824
  span: 5
  sdi-12_address: "3"
  depth: 18
  node: B
  field: LINEAR_CORN

- hash: "017"
  treatment: 4
  plot_number: 5007
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR5007B43024
  span: 5
  sdi-12_address: "4"
  depth: 30
  node: B
  field: LINEAR_CORN

- hash: "018"
  treatment: 4
  plot_number: 5007
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR5007B44224
  span: 5
  sdi-12_address: "5"
  depth: 42
  node: B
  field: LINEAR_CORN

- hash: "019"
  treatment: 1
  plot_number: 5006
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR5006B10624
  span: 5
  sdi-12_address: "6"
  depth: 6
  node: B
  field: LINEAR_CORN

- hash: "020"
  treatment: 1
  plot_number: 5006
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR5006B11824
  span: 5
  sdi-12_address: "7"
  depth: 18
  node: B
  field: LINEAR_CORN

- hash: "021"
  treatment: 1
  plot_number: 5006
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR5006B13024
  span: 5
  sdi-12_address: "8"
  depth: 30
  node: B
  field: LINEAR_CORN

- hash: "022"
  treatment: 1
  plot_number: 5006
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR5006B14224
  span: 5
  sdi-12_address: "9"
  depth: 42
  node: B
  field: LINEAR_CORN

- hash: "023"
  treatment: 2
  plot_number: 5012
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR5012B20624
  span: 5
  sdi-12_address: "a"
  depth: 6
  node: B
  field: LINEAR_CORN

- hash: "024"
  treatment: 2
  plot_number: 5012
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR5012B21824
  span: 5
  sdi-12_address: "b"
  depth: 18
  node: B
  field: LINEAR_CORN

- hash: "025"
  treatment: 2
  plot_number: 5012
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR5012B23024
  span: 5
  sdi-12_address: "c"
  depth: 30
  node: B
  field: LINEAR_CORN

# LINEAR_CORN Field Sensors (Node A)
# Total Sensors: 11 (3 IRT, 8 TDR)
- hash: "032"
  treatment: 1
  plot_number: 5023
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: IRT5023A1xx24
  span: 5
  sdi-12_address: "0"
  depth: 
  node: A
  field: LINEAR_CORN

- hash: "033"
  treatment: 3
  plot_number: 5020
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: IRT5020A3xx24
  span: 5
  sdi-12_address: "1"
  depth: 
  node: A
  field: LINEAR_CORN

- hash: "034"
  treatment: 3
  plot_number: 5018
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: IRT5018A3xx24
  span: 5
  sdi-12_address: "9"
  depth: 
  node: A
  field: LINEAR_CORN

- hash: "035"
  treatment: 1
  plot_number: 5023
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR5023A10624
  span: 5
  sdi-12_address: "2"
  depth: 6
  node: A
  field: LINEAR_CORN

- hash: "036"
  treatment: 1
  plot_number: 5023
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR5023A11824
  span: 5
  sdi-12_address: "3"
  depth: 18
  node: A
  field: LINEAR_CORN

- hash: "037"
  treatment: 1
  plot_number: 5023
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR5023A13024
  span: 5
  sdi-12_address: "4"
  depth: 30
  node: A
  field: LINEAR_CORN

- hash: "038"
  treatment: 1
  plot_number: 5023
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR5023A14224
  span: 5
  sdi-12_address: "5"
  depth: 42
  node: A
  field: LINEAR_CORN

- hash: "039"
  treatment: 2
  plot_number: 5026
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR5026A20624
  span: 5
  sdi-12_address: "6"
  depth: 6
  node: A
  field: LINEAR_CORN

- hash: "040"
  treatment: 2
  plot_number: 5026
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR5026A21824
  span: 5
  sdi-12_address: "7"
  depth: 18
  node: A
  field: LINEAR_CORN

- hash: "041"
  treatment: 2
  plot_number: 5026
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR5026A23824
  span: 5
  sdi-12_address: "8"
  depth: 38
  node: A
  field: LINEAR_CORN

- hash: "042"
  treatment: 4
  plot_number: 5027
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR5027A40624
  span: 5
  sdi-12_address: "a"
  depth: 6
  node: A
  field: LINEAR_CORN

- hash: "043"
  treatment: 4
  plot_number: 5027
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR5027A41824
  span: 5
  sdi-12_address: "b"
  depth: 18
  node: A
  field: LINEAR_CORN

- hash: "044"
  treatment: 4
  plot_number: 5027
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR5027A43024
  span: 5
  sdi-12_address: "c"
  depth: 30
  node: A
  field: LINEAR_CORN

  # New entries for DEN and SAP sensors in treatment 1 plots

- hash: "045"
  treatment: 1
  plot_number: 5006
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: DEN5006B1xx24
  span: 5
  node: B
  field: LINEAR_CORN

- hash: "046"
  treatment: 1
  plot_number: 5006
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: SAP5006B1xx24
  span: 5
  node: B
  field: LINEAR_CORN

- hash: "047"
  treatment: 1
  plot_number: 5010
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: DEN5010C1xx24
  span: 5
  node: C
  field: LINEAR_CORN

- hash: "048"
  treatment: 1
  plot_number: 5010
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: SAP5010C1xx24
  span: 5
  node: C
  field: LINEAR_CORN

- hash: "049"
  treatment: 1
  plot_number: 5023
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: DEN5023A1xx24
  span: 5
  node: A
  field: LINEAR_CORN

- hash: "050"
  treatment: 1
  plot_number: 5023
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: SAP5023A1xx24
  span: 5
  node: A
  field: LINEAR_CORN

  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  #         CORN FIELDS
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  #         SOYBEAN FIELDS
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  #         SOYBEAN FIELDS
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# LINEAR_SOYBEAN Field Sensors (Node A)
# Total Sensors: 8 (2 IRT, 6 TDR)
- hash: "051"
  treatment: 2
  plot_number: 2001
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: IRT2001A2xx24
  span: 5
  sdi-12_address: "0"
  depth: 
  node: A
  field: LINEAR_SOYBEAN

- hash: "052"
  treatment: 2
  plot_number: 2001
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR2001A20624
  span: 5
  sdi-12_address: "2"
  depth: 6
  node: A
  field: LINEAR_SOYBEAN

- hash: "053"
  treatment: 2
  plot_number: 2001
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR2001A21824
  span: 5
  sdi-12_address: "1"
  depth: 18
  node: A
  field: LINEAR_SOYBEAN

- hash: "054"
  treatment: 2
  plot_number: 2001
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR2001A23024
  span: 5
  sdi-12_address: "3"
  depth: 30
  node: A
  field: LINEAR_SOYBEAN

- hash: "055"
  treatment: 4
  plot_number: 2009
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR2009A40624
  span: 5
  sdi-12_address: "4"
  depth: 6
  node: A
  field: LINEAR_SOYBEAN

- hash: "056"
  treatment: 4
  plot_number: 2009
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR2009A41824
  span: 5
  sdi-12_address: "5"
  depth: 18
  node: A
  field: LINEAR_SOYBEAN

- hash: "057"
  treatment: 4
  plot_number: 2009
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: TDR2009A43024
  span: 5
  sdi-12_address: "6"
  depth: 30
  node: A
  field: LINEAR_SOYBEAN

- hash: "058"
  treatment: 3
  plot_number: 2003
  project_id: crop2cloud24
  dataset_id: node_a
  sensor_id: IRT2003A3xx24
  span: 5
  sdi-12_address: "7"
  depth: 
  node: A
  field: LINEAR_SOYBEAN

# LINEAR_SOYBEAN Field Sensors (Node B)
# Total Sensors: 11 (2 IRT, 9 TDR)
- hash: "059"
  treatment: 2
  plot_number: 2011
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: IRT2011B2xx24
  span: 5
  sdi-12_address: "0"
  depth: 
  node: B
  field: LINEAR_SOYBEAN

- hash: "060"
  treatment: 2
  plot_number: 2011
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR2011B20624
  span: 5
  sdi-12_address: "1"
  depth: 6
  node: B
  field: LINEAR_SOYBEAN

- hash: "061"
  treatment: 2
  plot_number: 2011
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR2011B21824
  span: 5
  sdi-12_address: "2"
  depth: 18
  node: B
  field: LINEAR_SOYBEAN

- hash: "062"
  treatment: 2
  plot_number: 2011
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR2011B23024
  span: 5
  sdi-12_address: "3"
  depth: 30
  node: B
  field: LINEAR_SOYBEAN

- hash: "063"
  treatment: 1
  plot_number: 2006
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: IRT2006B1xx24
  span: 5
  sdi-12_address: "b"
  depth: 
  node: B
  field: LINEAR_SOYBEAN

- hash: "064"
  treatment: 1
  plot_number: 2006
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR2006B10624
  span: 5
  sdi-12_address: "4"
  depth: 6
  node: B
  field: LINEAR_SOYBEAN

- hash: "065"
  treatment: 1
  plot_number: 2006
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR2006B11824
  span: 5
  sdi-12_address: "5"
  depth: 18
  node: B
  field: LINEAR_SOYBEAN

- hash: "066"
  treatment: 1
  plot_number: 2006
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR2006B13024
  span: 5
  sdi-12_address: "6"
  depth: 30
  node: B
  field: LINEAR_SOYBEAN

- hash: "067"
  treatment: 1
  plot_number: 2006
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR2006B14224
  span: 5
  sdi-12_address: "7"
  depth: 42
  node: B
  field: LINEAR_SOYBEAN

- hash: "068"
  treatment: 4
  plot_number: 2012
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR2012B40624
  span: 5
  sdi-12_address: "8"
  depth: 6
  node: B
  field: LINEAR_SOYBEAN

- hash: "069"
  treatment: 4
  plot_number: 2012
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR2012B41824
  span: 5
  sdi-12_address: "9"
  depth: 18
  node: B
  field: LINEAR_SOYBEAN

- hash: "070"
  treatment: 4
  plot_number: 2012
  project_id: crop2cloud24
  dataset_id: node_b
  sensor_id: TDR2012B43024
  span: 5
  sdi-12_address: "a"
  depth: 30
  node: B
  field: LINEAR_SOYBEAN

# LINEAR_SOYBEAN Field Sensors (Node C)
# Total Sensors: 10 (2 IRT, 8 TDR)
- hash: "071"
  treatment: 1
  plot_number: 2023
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: IRT2023C1xx24
  span: 5
  sdi-12_address: "0"
  depth: 
  node: C
  field: LINEAR_SOYBEAN

- hash: "072"
  treatment: 1
  plot_number: 2023
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR2023C10624
  span: 5
  sdi-12_address: "1"
  depth: 6
  node: C
  field: LINEAR_SOYBEAN

- hash: "073"
  treatment: 1
  plot_number: 2023
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR2023C11824
  span: 5
  sdi-12_address: "2"
  depth: 18
  node: C
  field: LINEAR_SOYBEAN

- hash: "074"
  treatment: 1
  plot_number: 2023
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR2023C13024
  span: 5
  sdi-12_address: "3"
  depth: 30
  node: C
  field: LINEAR_SOYBEAN

- hash: "075"
  treatment: 1
  plot_number: 2023
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR2023C14224
  span: 5
  sdi-12_address: "4"
  depth: 42
  node: C
  field: LINEAR_SOYBEAN

- hash: "076"
  treatment: 1
  plot_number: 2015
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: IRT2015C1xx24
  span: 5
  sdi-12_address: "6"
  depth: 
  node: C
  field: LINEAR_SOYBEAN

- hash: "077"
  treatment: 1
  plot_number: 2015
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR2015C10624
  span: 5
  sdi-12_address: "7"
  depth: 6
  node: C
  field: LINEAR_SOYBEAN

- hash: "078"
  treatment: 1
  plot_number: 2015
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR2015C11824
  span: 5
  sdi-12_address: "8"
  depth: 18
  node: C
  field: LINEAR_SOYBEAN

- hash: "079"
  treatment: 1
  plot_number: 2015
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR2015C13024
  span: 5
  sdi-12_address: "9"
  depth: 30
  node: C
  field: LINEAR_SOYBEAN

- hash: "080"
  treatment: 1
  plot_number: 2015
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR2015C14224
  span: 5
  sdi-12_address: "a"
  depth: 42
  node: C
  field: LINEAR_SOYBEAN

- hash: "081"
  treatment: 3
  plot_number: 2018
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: IRT2018C3xx24
  span: 5
  sdi-12_address: "b"
  depth: 
  node: C
  field: LINEAR_SOYBEAN

- hash: "082"
  treatment: 3
  plot_number: 2020
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: IRT2020C3xx24
  span: 5
  sdi-12_address: "g"
  depth: 
  node: C
  field: LINEAR_SOYBEAN

- hash: "083"
  treatment: 2
  plot_number: 2026
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR2026C23024
  span: 5
  sdi-12_address: "c"
  depth: 30
  node: C
  field: LINEAR_SOYBEAN

- hash: "084"
  treatment: 2
  plot_number: 2026
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR2026C21824
  span: 5
  sdi-12_address: "d"
  depth: 18
  node: C
  field: LINEAR_SOYBEAN

- hash: "085"
  treatment: 2
  plot_number: 2026
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: TDR2026C20624
  span: 5
  sdi-12_address: "e"
  depth: 6
  node: C
  field: LINEAR_SOYBEAN

- hash: "086"
  treatment: 2
  plot_number: 2026
  project_id: crop2cloud24
  dataset_id: node_c
  sensor_id: IRT2026C2xx24
  span: 5
  sdi-12_address: "h"
  depth: 
  node: C
  field: LINEAR_SOYBEAN

  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  #         SOYBEAN FIELDS
  #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
        </content>
    </file>
</directory>
<directory name="analysis-2024-07-19">
</directory>
    <directory name="data-07-19-2024">
        <file>
        <name>LINEAR_CORN_trt1_plot_5006_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_CORN_trt1_plot_5006_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_CORN_trt1_plot_5010_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_CORN_trt1_plot_5010_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_CORN_trt1_plot_5023_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_CORN_trt1_plot_5023_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_CORN_trt2_plot_5003_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_CORN_trt2_plot_5003_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_CORN_trt2_plot_5012_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_CORN_trt2_plot_5012_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_CORN_trt2_plot_5026_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_CORN_trt2_plot_5026_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_CORN_trt3_plot_5001_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_CORN_trt3_plot_5001_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_CORN_trt3_plot_5018_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_CORN_trt3_plot_5018_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_CORN_trt3_plot_5020_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_CORN_trt3_plot_5020_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_CORN_trt4_plot_5007_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_CORN_trt4_plot_5007_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_CORN_trt4_plot_5009_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_CORN_trt4_plot_5009_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_CORN_trt4_plot_5027_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_CORN_trt4_plot_5027_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2006_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_SOYBEAN_trt1_plot_2006_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2015_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_SOYBEAN_trt1_plot_2015_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2023_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_SOYBEAN_trt1_plot_2023_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2001_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_SOYBEAN_trt2_plot_2001_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2011_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_SOYBEAN_trt2_plot_2011_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2026_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_SOYBEAN_trt2_plot_2026_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2003_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_SOYBEAN_trt3_plot_2003_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2018_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_SOYBEAN_trt3_plot_2018_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2020_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_SOYBEAN_trt3_plot_2020_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_SOYBEAN_trt4_plot_2009_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_SOYBEAN_trt4_plot_2009_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>LINEAR_SOYBEAN_trt4_plot_2012_20240719.csv</name>
        <path>analysis-2024-07-19\data-07-19-2024\LINEAR_SOYBEAN_trt4_plot_2012_20240719.csv</path>
        <content>Full content not provided</content>
    </file>
    </directory>
    <directory name="recommendations-07-19-2024">
        <file>
        <name>fuzzy-trt-1.csv</name>
        <path>analysis-2024-07-19\recommendations-07-19-2024\fuzzy-trt-1.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>other_recommendations.csv</name>
        <path>analysis-2024-07-19\recommendations-07-19-2024\other_recommendations.csv</path>
        <content>Full content not provided</content>
    </file>
    </directory>
<directory name="analysis-2024-07-26">
</directory>
    <directory name="data">
    </directory>
        <directory name="data-2024-07-26">
            <file>
        <name>LINEAR_CORN_trt1_plot_5006_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_CORN_trt1_plot_5006_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt1_plot_5010_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_CORN_trt1_plot_5010_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt1_plot_5023_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_CORN_trt1_plot_5023_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5003_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_CORN_trt2_plot_5003_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5012_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_CORN_trt2_plot_5012_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5026_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_CORN_trt2_plot_5026_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5001_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_CORN_trt3_plot_5001_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5018_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_CORN_trt3_plot_5018_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5020_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_CORN_trt3_plot_5020_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5007_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_CORN_trt4_plot_5007_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5009_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_CORN_trt4_plot_5009_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5027_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_CORN_trt4_plot_5027_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2006_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_SOYBEAN_trt1_plot_2006_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2015_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_SOYBEAN_trt1_plot_2015_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2023_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_SOYBEAN_trt1_plot_2023_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2001_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_SOYBEAN_trt2_plot_2001_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2011_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_SOYBEAN_trt2_plot_2011_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2026_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_SOYBEAN_trt2_plot_2026_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2003_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_SOYBEAN_trt3_plot_2003_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2018_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_SOYBEAN_trt3_plot_2018_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2020_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_SOYBEAN_trt3_plot_2020_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt4_plot_2009_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_SOYBEAN_trt4_plot_2009_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt4_plot_2012_20240726.csv</name>
        <path>analysis-2024-07-26\data\data-2024-07-26\LINEAR_SOYBEAN_trt4_plot_2012_20240726.csv</path>
        <content>Full content not provided</content>
    </file>
        </directory>
    <directory name="recommendations">
        <file>
        <name>fuzzy-trt-1.csv</name>
        <path>analysis-2024-07-26\recommendations\fuzzy-trt-1.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>Oher Treatments.csv</name>
        <path>analysis-2024-07-26\recommendations\Oher Treatments.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>recommendations.csv</name>
        <path>analysis-2024-07-26\recommendations\recommendations.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>Treatment1-Corn-Soybeans.csv</name>
        <path>analysis-2024-07-26\recommendations\Treatment1-Corn-Soybeans.csv</path>
        <content>Full content not provided</content>
    </file>
    </directory>
<directory name="analysis-2024-08-05">
</directory>
    <directory name="data">
    </directory>
        <directory name="data-2024-08-05">
            <file>
        <name>LINEAR_CORN_trt1_plot_5006_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_CORN_trt1_plot_5006_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt1_plot_5010_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_CORN_trt1_plot_5010_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt1_plot_5023_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_CORN_trt1_plot_5023_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5003_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_CORN_trt2_plot_5003_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5012_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_CORN_trt2_plot_5012_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5026_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_CORN_trt2_plot_5026_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5001_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_CORN_trt3_plot_5001_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5018_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_CORN_trt3_plot_5018_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5020_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_CORN_trt3_plot_5020_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5007_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_CORN_trt4_plot_5007_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5009_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_CORN_trt4_plot_5009_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5027_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_CORN_trt4_plot_5027_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2006_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_SOYBEAN_trt1_plot_2006_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2015_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_SOYBEAN_trt1_plot_2015_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2023_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_SOYBEAN_trt1_plot_2023_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2001_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_SOYBEAN_trt2_plot_2001_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2011_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_SOYBEAN_trt2_plot_2011_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2026_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_SOYBEAN_trt2_plot_2026_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2003_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_SOYBEAN_trt3_plot_2003_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2018_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_SOYBEAN_trt3_plot_2018_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2020_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_SOYBEAN_trt3_plot_2020_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt4_plot_2009_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_SOYBEAN_trt4_plot_2009_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt4_plot_2012_20240805.csv</name>
        <path>analysis-2024-08-05\data\data-2024-08-05\LINEAR_SOYBEAN_trt4_plot_2012_20240805.csv</path>
        <content>Full content not provided</content>
    </file>
        </directory>
    <directory name="recommendations">
        <file>
        <name>fuzzy-trt-1.csv</name>
        <path>analysis-2024-08-05\recommendations\fuzzy-trt-1.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>recommendations.csv</name>
        <path>analysis-2024-08-05\recommendations\recommendations.csv</path>
        <content>Full content not provided</content>
    </file>
    </directory>
<directory name="analysis-2024-08-09">
</directory>
    <directory name="data">
    </directory>
        <directory name="data-2024-08-09">
            <file>
        <name>LINEAR_CORN_trt1_plot_5006_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_CORN_trt1_plot_5006_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt1_plot_5010_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_CORN_trt1_plot_5010_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt1_plot_5023_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_CORN_trt1_plot_5023_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5003_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_CORN_trt2_plot_5003_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5012_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_CORN_trt2_plot_5012_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5026_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_CORN_trt2_plot_5026_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5001_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_CORN_trt3_plot_5001_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5018_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_CORN_trt3_plot_5018_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5020_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_CORN_trt3_plot_5020_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5007_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_CORN_trt4_plot_5007_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5009_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_CORN_trt4_plot_5009_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5027_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_CORN_trt4_plot_5027_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2006_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_SOYBEAN_trt1_plot_2006_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2015_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_SOYBEAN_trt1_plot_2015_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2023_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_SOYBEAN_trt1_plot_2023_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2001_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_SOYBEAN_trt2_plot_2001_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2011_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_SOYBEAN_trt2_plot_2011_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2026_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_SOYBEAN_trt2_plot_2026_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2003_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_SOYBEAN_trt3_plot_2003_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2018_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_SOYBEAN_trt3_plot_2018_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2020_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_SOYBEAN_trt3_plot_2020_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt4_plot_2009_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_SOYBEAN_trt4_plot_2009_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt4_plot_2012_20240809.csv</name>
        <path>analysis-2024-08-09\data\data-2024-08-09\LINEAR_SOYBEAN_trt4_plot_2012_20240809.csv</path>
        <content>Full content not provided</content>
    </file>
        </directory>
    <directory name="recommendations">
        <file>
        <name>fuzzy-trt-1.csv</name>
        <path>analysis-2024-08-09\recommendations\fuzzy-trt-1.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>recommendations.csv</name>
        <path>analysis-2024-08-09\recommendations\recommendations.csv</path>
        <content>Full content not provided</content>
    </file>
    </directory>
<directory name="analysis-2024-08-29">
</directory>
    <directory name="data">
        <file>
        <name>CanopyTemp_1368_SDI1SoyBlockA_SDIC_BlockC.csv</name>
        <path>analysis-2024-08-29\data\CanopyTemp_1368_SDI1SoyBlockA_SDIC_BlockC.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>CanopyTemp_plot_SDI1SoyBlockA_SDIC_BlockC.png</name>
        <path>analysis-2024-08-29\data\CanopyTemp_plot_SDI1SoyBlockA_SDIC_BlockC.png</path>
        <content>Full content not provided</content>
    </file>
    </directory>
        <directory name="data-2024-08-29">
            <file>
        <name>LINEAR_CORN_trt1_plot_5006_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_CORN_trt1_plot_5006_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt1_plot_5010_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_CORN_trt1_plot_5010_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt1_plot_5023_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_CORN_trt1_plot_5023_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5003_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_CORN_trt2_plot_5003_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5012_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_CORN_trt2_plot_5012_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5026_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_CORN_trt2_plot_5026_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5001_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_CORN_trt3_plot_5001_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5018_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_CORN_trt3_plot_5018_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5020_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_CORN_trt3_plot_5020_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5007_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_CORN_trt4_plot_5007_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5009_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_CORN_trt4_plot_5009_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5027_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_CORN_trt4_plot_5027_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2006_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_SOYBEAN_trt1_plot_2006_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2015_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_SOYBEAN_trt1_plot_2015_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2023_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_SOYBEAN_trt1_plot_2023_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2001_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_SOYBEAN_trt2_plot_2001_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2011_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_SOYBEAN_trt2_plot_2011_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2026_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_SOYBEAN_trt2_plot_2026_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2003_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_SOYBEAN_trt3_plot_2003_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2018_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_SOYBEAN_trt3_plot_2018_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2020_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_SOYBEAN_trt3_plot_2020_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt4_plot_2009_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_SOYBEAN_trt4_plot_2009_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt4_plot_2012_20240829.csv</name>
        <path>analysis-2024-08-29\data\data-2024-08-29\LINEAR_SOYBEAN_trt4_plot_2012_20240829.csv</path>
        <content>Full content not provided</content>
    </file>
        </directory>
    <directory name="recommendations">
        <file>
        <name>fuzzy-trt-1.csv</name>
        <path>analysis-2024-08-29\recommendations\fuzzy-trt-1.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>recommendations.csv</name>
        <path>analysis-2024-08-29\recommendations\recommendations.csv</path>
        <content>Full content not provided</content>
    </file>
    </directory>
<directory name="analysis-2024-08-30">
</directory>
    <directory name="data">
    </directory>
        <directory name="data-2024-08-30">
            <file>
        <name>LINEAR_CORN_trt1_plot_5006_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_CORN_trt1_plot_5006_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt1_plot_5010_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_CORN_trt1_plot_5010_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt1_plot_5023_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_CORN_trt1_plot_5023_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5003_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_CORN_trt2_plot_5003_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5012_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_CORN_trt2_plot_5012_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5026_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_CORN_trt2_plot_5026_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5001_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_CORN_trt3_plot_5001_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5018_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_CORN_trt3_plot_5018_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5020_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_CORN_trt3_plot_5020_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5007_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_CORN_trt4_plot_5007_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5009_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_CORN_trt4_plot_5009_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5027_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_CORN_trt4_plot_5027_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2006_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_SOYBEAN_trt1_plot_2006_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2015_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_SOYBEAN_trt1_plot_2015_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2023_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_SOYBEAN_trt1_plot_2023_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2001_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_SOYBEAN_trt2_plot_2001_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2011_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_SOYBEAN_trt2_plot_2011_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2026_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_SOYBEAN_trt2_plot_2026_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2003_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_SOYBEAN_trt3_plot_2003_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2018_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_SOYBEAN_trt3_plot_2018_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2020_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_SOYBEAN_trt3_plot_2020_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt4_plot_2009_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_SOYBEAN_trt4_plot_2009_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt4_plot_2012_20240830.csv</name>
        <path>analysis-2024-08-30\data\data-2024-08-30\LINEAR_SOYBEAN_trt4_plot_2012_20240830.csv</path>
        <content>Full content not provided</content>
    </file>
        </directory>
    <directory name="recommendations">
        <file>
        <name>fuzzy-trt-1.csv</name>
        <path>analysis-2024-08-30\recommendations\fuzzy-trt-1.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>recommendations.csv</name>
        <path>analysis-2024-08-30\recommendations\recommendations.csv</path>
        <content>Full content not provided</content>
    </file>
    </directory>
<directory name="analysis-2024-09-15">
</directory>
    <directory name="data">
    </directory>
        <directory name="data-2024-09-15">
            <file>
        <name>LINEAR_CORN_trt1_plot_5006_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_CORN_trt1_plot_5006_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt1_plot_5010_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_CORN_trt1_plot_5010_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt1_plot_5023_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_CORN_trt1_plot_5023_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5003_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_CORN_trt2_plot_5003_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5012_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_CORN_trt2_plot_5012_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt2_plot_5026_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_CORN_trt2_plot_5026_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5001_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_CORN_trt3_plot_5001_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5018_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_CORN_trt3_plot_5018_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt3_plot_5020_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_CORN_trt3_plot_5020_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5007_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_CORN_trt4_plot_5007_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5009_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_CORN_trt4_plot_5009_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_CORN_trt4_plot_5027_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_CORN_trt4_plot_5027_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2006_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_SOYBEAN_trt1_plot_2006_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2015_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_SOYBEAN_trt1_plot_2015_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt1_plot_2023_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_SOYBEAN_trt1_plot_2023_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2001_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_SOYBEAN_trt2_plot_2001_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2011_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_SOYBEAN_trt2_plot_2011_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt2_plot_2026_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_SOYBEAN_trt2_plot_2026_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2003_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_SOYBEAN_trt3_plot_2003_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2018_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_SOYBEAN_trt3_plot_2018_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt3_plot_2020_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_SOYBEAN_trt3_plot_2020_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt4_plot_2009_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_SOYBEAN_trt4_plot_2009_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
            <file>
        <name>LINEAR_SOYBEAN_trt4_plot_2012_20240915.csv</name>
        <path>analysis-2024-09-15\data\data-2024-09-15\LINEAR_SOYBEAN_trt4_plot_2012_20240915.csv</path>
        <content>Full content not provided</content>
    </file>
        </directory>
    <directory name="recommendations">
        <file>
        <name>fuzzy-trt-1.csv</name>
        <path>analysis-2024-09-15\recommendations\fuzzy-trt-1.csv</path>
        <content>Full content not provided</content>
    </file>
        <file>
        <name>recommendations.csv</name>
        <path>analysis-2024-09-15\recommendations\recommendations.csv</path>
        <content>Full content not provided</content>
    </file>
    </directory>
<directory name="images">
    <file>
        <name>fuzzy_control_system.png</name>
        <path>images\fuzzy_control_system.png</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>fuzzy_output_plot_2006.png</name>
        <path>images\fuzzy_output_plot_2006.png</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>fuzzy_output_plot_2015.png</name>
        <path>images\fuzzy_output_plot_2015.png</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>fuzzy_output_plot_2023.png</name>
        <path>images\fuzzy_output_plot_2023.png</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>fuzzy_output_plot_5006.png</name>
        <path>images\fuzzy_output_plot_5006.png</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>fuzzy_output_plot_5010.png</name>
        <path>images\fuzzy_output_plot_5010.png</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>fuzzy_output_plot_5023.png</name>
        <path>images\fuzzy_output_plot_5023.png</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>membership_functions.png</name>
        <path>images\membership_functions.png</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>recent_data_plot_2006.png</name>
        <path>images\recent_data_plot_2006.png</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>recent_data_plot_2015.png</name>
        <path>images\recent_data_plot_2015.png</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>recent_data_plot_2023.png</name>
        <path>images\recent_data_plot_2023.png</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>recent_data_plot_5006.png</name>
        <path>images\recent_data_plot_5006.png</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>recent_data_plot_5010.png</name>
        <path>images\recent_data_plot_5010.png</path>
        <content>Full content not provided</content>
    </file>
    <file>
        <name>recent_data_plot_5023.png</name>
        <path>images\recent_data_plot_5023.png</path>
        <content>Full content not provided</content>
    </file>
</directory>
<directory name="src">
    <file>
        <name>create_bq_data_table.py</name>
        <path>src\create_bq_data_table.py</path>
        <content>
import os
from google.cloud import bigquery
from google.oauth2 import service_account
from dotenv import load_dotenv
import yaml
from google.api_core import exceptions  # Correct import for NotFound exception

def load_sensor_mapping(file_path):
    try:
        with open(file_path, 'r') as file:
            return yaml.safe_load(file)
    except FileNotFoundError:
        print(f"Error: Sensor mapping file not found: {file_path}")
        raise
    except yaml.YAMLError as e:
        print(f"Error: Error parsing YAML file: {e}")
        raise

def create_bigquery_client():
    load_dotenv()
    credentials_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
    
    if not credentials_path:
        print("Error: GOOGLE_APPLICATION_CREDENTIALS not set in .env file")
        raise ValueError("GOOGLE_APPLICATION_CREDENTIALS not set in .env file")
    
    try:
        credentials = service_account.Credentials.from_service_account_file(
            credentials_path,
            scopes=["https://www.googleapis.com/auth/cloud-platform"],
        )
        return bigquery.Client(credentials=credentials, project=credentials.project_id)
    except Exception as e:
        print(f"Error: Error creating BigQuery client: {e}")
        raise

def ensure_dataset_exists(client, dataset_id, delete_existing=False):
    dataset_ref = client.dataset(dataset_id)
    try:
        if delete_existing:
            client.delete_dataset(dataset_ref, delete_contents=True, not_found_ok=True)
            print(f"Deleted existing dataset {dataset_id}")
        dataset = client.get_dataset(dataset_ref)
        print(f"Dataset {dataset_id} already exists")
    except exceptions.NotFound:
        # If dataset is not found, create it
        dataset = bigquery.Dataset(dataset_ref)
        dataset = client.create_dataset(dataset)
        print(f"Created dataset {dataset_id}")

def create_or_update_schemaless_table(client, table_id, delete_existing=False):
    try:
        if delete_existing:
            client.delete_table(table_id, not_found_ok=True)
            print(f"Deleted existing table {table_id}")
        
        try:
            table = client.get_table(table_id)
            print(f"Table {table_id} already exists. No schema changes needed.")
        except exceptions.NotFound:
            # If table is not found, create a new table without schema
            table = bigquery.Table(table_id)
            table = client.create_table(table)
            print(f"Created schemaless table {table_id}")
    
    except Exception as e:
        print(f"Error: Error creating or updating table {table_id}: {e}")
        raise

def process_sensor_mapping(client, sensor_mapping, delete_existing=False):
    field_treatment_plot_sensor_map = {}
    for sensor in sensor_mapping:
        field = sensor['field']
        treatment = sensor['treatment']
        plot_number = sensor['plot_number']
        sensor_id = sensor['sensor_id']
        
        if field not in field_treatment_plot_sensor_map:
            field_treatment_plot_sensor_map[field] = {}
        if treatment not in field_treatment_plot_sensor_map[field]:
            field_treatment_plot_sensor_map[field][treatment] = {}
        if plot_number not in field_treatment_plot_sensor_map[field][treatment]:
            field_treatment_plot_sensor_map[field][treatment][plot_number] = set()
        
        field_treatment_plot_sensor_map[field][treatment][plot_number].add(sensor_id)
    
    for field, treatments in field_treatment_plot_sensor_map.items():
        for treatment, plots in treatments.items():
            dataset_id = f"{field}_trt{treatment}"
            ensure_dataset_exists(client, dataset_id, delete_existing)
            
            for plot_number, sensor_ids in plots.items():
                table_id = f"{client.project}.{dataset_id}.plot_{plot_number}"
                create_or_update_schemaless_table(client, table_id, delete_existing)

def main():
    try:
        yaml_path = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\sensor_mapping.yaml"
        sensor_mapping = load_sensor_mapping(yaml_path)
        client = create_bigquery_client()

        delete_flag = input("Do you want to delete existing tables/datasets before creating new ones? (y/n): ").lower() == 'y'

        if delete_flag:
            confirmation1 = input("Are you sure you want to delete existing tables/datasets? (y/n): ").lower()
            if confirmation1 == 'y':
                confirmation2 = input("This action is irreversible. Type 'y' again to confirm deletion: ").lower()
                if confirmation2 == 'y':
                    print("Deletion confirmed. Proceeding with table creation (including deletion of existing ones).")
                    process_sensor_mapping(client, sensor_mapping, delete_existing=True)
                else:
                    print("Deletion cancelled. Proceeding with table creation without deleting existing ones.")
                    process_sensor_mapping(client, sensor_mapping, delete_existing=False)
            else:
                print("Deletion cancelled. Proceeding with table creation without deleting existing ones.")
                process_sensor_mapping(client, sensor_mapping, delete_existing=False)
        else:
            print("Proceeding with table creation without deleting existing ones.")
            process_sensor_mapping(client, sensor_mapping, delete_existing=False)

        print("Process completed successfully")
    except Exception as e:
        print(f"An error occurred: {e}")

if __name__ == "__main__":
    main()

        </content>
    </file>
    <file>
        <name>cwsi_th1.py</name>
        <path>src\cwsi_th1.py</path>
        <content>
# src/cwsi_th1.py

import os
import pandas as pd
import numpy as np
import logging
from datetime import time, timedelta

# Configuration
STEFAN_BOLTZMANN = 5.67e-8
CP = 1005
K = 0.41
CROP_HEIGHTS = {
    'CORN': 2.845,  # Adjust this value as needed
    'SOYBEAN': 0.914  # Adjust this value as needed
}
SURFACE_ALBEDO = 0.23

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def celsius_to_kelvin(temp_celsius):
    return temp_celsius + 273.15

def saturated_vapor_pressure(temperature_celsius):
    return 0.6108 * np.exp(17.27 * temperature_celsius / (temperature_celsius + 237.3))

def vapor_pressure_deficit(temperature_celsius, relative_humidity):
    es = saturated_vapor_pressure(temperature_celsius)
    ea = es * (relative_humidity / 100)
    return es - ea

def net_radiation(solar_radiation, air_temp_celsius, canopy_temp_celsius, surface_albedo=0.23, emissivity_a=0.85, emissivity_c=0.98):
    air_temp_kelvin = celsius_to_kelvin(air_temp_celsius)
    canopy_temp_kelvin = celsius_to_kelvin(canopy_temp_celsius)
    Rns = (1 - surface_albedo) * solar_radiation
    Rnl = emissivity_c * STEFAN_BOLTZMANN * canopy_temp_kelvin**4 - emissivity_a * STEFAN_BOLTZMANN * air_temp_kelvin**4
    return Rns - Rnl

def soil_heat_flux(net_radiation):
    return net_radiation * 0.1

def aerodynamic_resistance(wind_speed, measurement_height, zero_plane_displacement, roughness_length):
    return (np.log((measurement_height - zero_plane_displacement) / roughness_length) * 
            np.log((measurement_height - zero_plane_displacement) / (roughness_length * 0.1))) / (K**2 * wind_speed)

def psychrometric_constant(atmospheric_pressure_pa):
    return (CP * atmospheric_pressure_pa) / (0.622 * 2.45e6)

def slope_saturation_vapor_pressure(temperature_celsius):
    return 4098 * saturated_vapor_pressure(temperature_celsius) / (temperature_celsius + 237.3)**2

def convert_wind_speed(u3, crop_height):
    z0 = 0.1 * crop_height
    return u3 * (np.log(2/z0) / np.log(3/z0))

def calculate_cwsi_th1(row, crop_height, surface_albedo=0.23):
    Ta = row['Ta_2m_Avg']
    RH = row['RH_2m_Avg']
    Rs = row['Solar_2m_Avg']
    u3 = row['WndAveSpd_3m']
    P = row['PresAvg_1pnt5m'] * 100
    Tc = row['canopy_temp']
    
    u2 = convert_wind_speed(u3, crop_height)
    
    VPD = vapor_pressure_deficit(Ta, RH)
    Rn = net_radiation(Rs, Ta, Tc, surface_albedo)
    G = soil_heat_flux(Rn)
    
    zero_plane_displacement = 0.67 * crop_height
    roughness_length = 0.123 * crop_height
    
    ra = aerodynamic_resistance(u2, 2, zero_plane_displacement, roughness_length)
     = psychrometric_constant(P)
     = slope_saturation_vapor_pressure(Ta)
    
     = P / (287.05 * celsius_to_kelvin(Ta))
    
    numerator = (Tc - Ta) - ((ra * (Rn - G)) / ( * CP)) + (VPD / )
    denominator = (( + ) * ra * (Rn - G)) / ( * CP * ) + (VPD / )
    
    if denominator == 0:
        return None
    
    cwsi = numerator / denominator
    return cwsi if 0 <= cwsi <= 2 else None

def process_csv_file(file_path):
    logger.info(f"Processing file: {file_path}")
    
    # Read the CSV file
    df = pd.read_csv(file_path, parse_dates=['TIMESTAMP'])
    
    # Determine crop type from filename
    crop_type = 'CORN' if 'CORN' in file_path.upper() else 'SOYBEAN'
    crop_height = CROP_HEIGHTS[crop_type]
    
    # Find the IRT column
    irt_column = next((col for col in df.columns if 'irt' in col.lower()), None)
    if not irt_column:
        logger.warning(f"No IRT column found in {file_path}")
        return None
    
    # Filter data between 12 PM and 5 PM
    df['time'] = df['TIMESTAMP'].dt.time
    mask = (df['time'] >= time(12, 0)) & (df['time'] <= time(17, 0))
    
    # Rename IRT column to 'canopy_temp' for CWSI calculation
    df.loc[mask, 'canopy_temp'] = df.loc[mask, irt_column]
    
    # Remove existing CWSI column if it exists
    if 'cwsi' in df.columns:
        df = df.drop(columns=['cwsi'])
    
    # Calculate CWSI only for the filtered rows
    df.loc[mask, 'cwsi'] = df.loc[mask].apply(lambda row: calculate_cwsi_th1(row, crop_height, SURFACE_ALBEDO), axis=1)
    
    # Remove temporary columns
    df = df.drop(columns=['time', 'canopy_temp'])
    
    # Log the range of computed CWSI values
    cwsi_values = df['cwsi'].dropna()
    if not cwsi_values.empty:
        logger.info(f"CWSI value ranges for {file_path}: Min={cwsi_values.min()}, Max={cwsi_values.max()}, Mean={cwsi_values.mean()}")
    else:
        logger.info(f"No CWSI values computed for {file_path}")
    
    # Save the updated DataFrame back to the same CSV file
    df.to_csv(file_path, index=False)
    logger.info(f"Updated CWSI values in file: {file_path}")
    
    return df

def main(input_folder):
    for root, _, files in os.walk(input_folder):
        for file in files:
            if file.endswith('.csv'):
                file_path = os.path.join(root, file)
                process_csv_file(file_path)

if __name__ == "__main__":
    input_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\data"
    main(input_folder)
        </content>
    </file>
    <file>
        <name>cwsi_th2_soybean.py</name>
        <path>src\cwsi_th2_soybean.py</path>
        <content>
# src/cwsi_soybean.py

import os
import pandas as pd
import numpy as np
import logging
from datetime import time, timedelta

# Configuration
REFERENCE_TEMP_CSV = r"C:\Users\bnsoh2\Downloads\CanopyTemp_1368_User.csv"

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def calculate_cwsi_soybean(row, reference_temp):
    """
    Calculate CWSI for soybeans using the reference temperature method.
    """
    Tc = row['canopy_temp']
    Ta = row['Ta_2m_Avg']
    Tc_ref = reference_temp

    if Tc_ref - Ta == 0:
        return None

    cwsi = (Tc - Tc_ref) / (Tc_ref - Ta)
    return max(0, min(cwsi, 1))  # Clip CWSI between 0 and 1

def process_csv_file(file_path, reference_df):
    logger.info(f"Processing file: {file_path}")
    
    # Read the CSV file
    df = pd.read_csv(file_path, parse_dates=['TIMESTAMP'])
    
    # Find the IRT column
    irt_column = next((col for col in df.columns if 'irt' in col.lower()), None)
    if not irt_column:
        logger.warning(f"No IRT column found in {file_path}")
        return None
    
    # Filter data between 12 PM and 5 PM
    df['time'] = df['TIMESTAMP'].dt.time
    mask = (df['time'] >= time(12, 0)) & (df['time'] <= time(17, 0))
    
    # Rename IRT column to 'canopy_temp' for CWSI calculation
    df.loc[mask, 'canopy_temp'] = df.loc[mask, irt_column]
    
    # Remove existing CWSI column if it exists
    if 'cwsi' in df.columns:
        df = df.drop(columns=['cwsi'])
    
    # Merge with reference temperature data
    df = pd.merge_asof(df, reference_df, on='TIMESTAMP', direction='nearest')
    
    # Calculate CWSI only for the filtered rows
    df.loc[mask, 'cwsi'] = df.loc[mask].apply(lambda row: calculate_cwsi_soybean(row, row['CanopyTemp_1368']), axis=1)
    
    # Remove temporary columns
    df = df.drop(columns=['time', 'canopy_temp', 'CanopyTemp_1368'])
    
    # Log the range of computed CWSI values
    cwsi_values = df['cwsi'].dropna()
    if not cwsi_values.empty:
        logger.info(f"CWSI value ranges for {file_path}: Min={cwsi_values.min()}, Max={cwsi_values.max()}, Mean={cwsi_values.mean()}")
    else:
        logger.info(f"No CWSI values computed for {file_path}")
    
    # Save the updated DataFrame back to the same CSV file
    df.to_csv(file_path, index=False)
    logger.info(f"Updated CWSI values in file: {file_path}")
    
    return df

def main(input_folder):
    # Load reference temperature data
    reference_df = pd.read_csv(REFERENCE_TEMP_CSV, parse_dates=['TIMESTAMP'])
    reference_df = reference_df[['TIMESTAMP', 'CanopyTemp_1368']]

    for root, _, files in os.walk(input_folder):
        for file in files:
            if file.endswith('.csv') and 'SOYBEAN' in file.upper():
                file_path = os.path.join(root, file)
                process_csv_file(file_path, reference_df)

if __name__ == "__main__":
    input_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\data"
    main(input_folder)
        </content>
    </file>
    <file>
        <name>dat_to_canopy_temp.py</name>
        <path>src\dat_to_canopy_temp.py</path>
        <content>
# src/dat_to_canopy_temp.py

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def compute_irt_temp(sb_temp_c, targ_mv, mC2, mC1, mC0, bC2, bC1, bC0):
    m = mC2 * sb_temp_c**2 + mC1 * sb_temp_c + mC0
    b = bC2 * sb_temp_c**2 + bC1 * sb_temp_c + bC0
    sb_temp_k = sb_temp_c + 273.15
    targ_temp_k = ((sb_temp_k**4) + m * targ_mv + b)**0.25
    return targ_temp_k - 273.15

def parse_dat_file_to_csv_with_temps(file_name, output_csv_1368_path, encoding='ISO-8859-1'):
    with open(file_name, "r", encoding=encoding) as file:
        lines = file.readlines()
    headers = lines[1].strip().split(",")
    data_lines = lines[4:]
    data = pd.DataFrame([line.strip().split(",") for line in data_lines], columns=headers)
    
    data.columns = data.columns.str.replace('"', "").str.replace("RECORD", "RecNbr")
    data.columns = data.columns.str.replace("_Avg", "")
    data = data.replace({"NAN": np.nan, '"NAN"': np.nan})
    data["TIMESTAMP"] = data["TIMESTAMP"].str.replace('"', "")
    
    for col in data.columns:
        if col != "TIMESTAMP":
            data[col] = pd.to_numeric(data[col], errors='coerce')
    
    data["TIMESTAMP"] = pd.to_datetime(data["TIMESTAMP"], errors="coerce")
    data = data[~data["TIMESTAMP"].isna()]
    
    # Define the coefficients for each sensor
    sensor_coefficients = {
        '1371': {'mC2': 77988.3, 'mC1': 8788030, 'mC0': 1620750000, 'bC2': 4298, 'bC1': 75731.6, 'bC0': -1569380},
        '1373': {'mC2': 84144.1, 'mC1': 8699300, 'mC0': 1583000000, 'bC2': 7825.17, 'bC1': 167578, 'bC0': 2947160},
        '1368': {'mC2': 119752, 'mC1': 10139500, 'mC0': 1787020000, 'bC2': 3884.19, 'bC1': 133998, 'bC0': -5140420},
        '1369': {'mC2': 118054, 'mC1': 10108500, 'mC0': 1790280000, 'bC2': 4019.44, 'bC1': 135393, 'bC0': -3958680},
        '1378': {'mC2': 110897, 'mC1': 9229850, 'mC0': 1632750000, 'bC2': 5598.07, 'bC1': 58091.5, 'bC0': 779020},
        '1374': {'mC2': 106949, 'mC1': 9387600, 'mC0': 1655170000, 'bC2': 4204.25, 'bC1': 102781, 'bC0': 450621},
        '1379': {'mC2': 80669, 'mC1': 8626800, 'mC0': 1580720000, 'bC2': 4165.68, 'bC1': 87436.7, 'bC0': 1813590},
        '1377': {'mC2': 105966, 'mC1': 9068660, 'mC0': 1634120000, 'bC2': 3816.74, 'bC1': 120624, 'bC0': 1025890}
    }
    
    # Compute canopy temperatures for each sensor
    for sensor, coeffs in sensor_coefficients.items():
        bod_col = f'BodC_{sensor}'
        tarm_col = f'TarmV_{sensor}'
        canopy_temp_col = f'CanopyTemp_{sensor}'
        if bod_col in data.columns and tarm_col in data.columns:
            data[canopy_temp_col] = compute_irt_temp(
                sb_temp_c=data[bod_col],
                targ_mv=data[tarm_col],
                mC2=coeffs['mC2'],
                mC1=coeffs['mC1'],
                mC0=coeffs['mC0'],
                bC2=coeffs['bC2'],
                bC1=coeffs['bC1'],
                bC0=coeffs['bC0']
            )
    
    # Resample to hourly data
    data_hourly = data.set_index("TIMESTAMP").resample('h').mean().reset_index()
    
    # Save the processed data for sensor 1368 to a CSV file
    canopy_temp_1368_df = data_hourly[["TIMESTAMP", "CanopyTemp_1368"]]
    canopy_temp_1368_df.to_csv(output_csv_1368_path, index=False)
    
    return data_hourly

def plot_canopy_temperatures(data, start_date, end_date, sensors, output_path):
    plt.figure(figsize=(14, 7))

    for sensor in sensors:
        canopy_temp_col = f'CanopyTemp_{sensor}'
        if canopy_temp_col in data.columns:
            plt.plot(data["TIMESTAMP"], data[canopy_temp_col], label=f"CanopyTemp {sensor}")

    plt.xlabel("Timestamp")
    plt.ylabel("Canopy Temperature (C)")
    plt.title(f"Canopy Temperatures from {start_date} to {end_date}")
    plt.legend()
    plt.grid(True)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(output_path)
    plt.close()

def process_dat_files(input_folder, output_folder):
    for root, _, files in os.walk(input_folder):
        for file in files:
            if file.endswith('.dat'):
                dat_file_path = os.path.join(root, file)
                output_csv_1368_path = os.path.join(output_folder, f"CanopyTemp_1368_{os.path.splitext(file)[0]}.csv")
                
                logger.info(f"Processing file: {dat_file_path}")
                processed_data_df = parse_dat_file_to_csv_with_temps(dat_file_path, output_csv_1368_path)
                
                # Plot canopy temperatures for the last 10 days for all sensors
                latest_timestamp = processed_data_df["TIMESTAMP"].max()
                start_date_last_10_days = latest_timestamp - pd.Timedelta(days=10)
                filtered_data_last_10_days = processed_data_df[
                    (processed_data_df["TIMESTAMP"] >= start_date_last_10_days) &
                    (processed_data_df["TIMESTAMP"] <= latest_timestamp)
                ]
                plot_output_path = os.path.join(output_folder, f"CanopyTemp_plot_{os.path.splitext(file)[0]}.png")
                plot_canopy_temperatures(filtered_data_last_10_days, start_date_last_10_days.date(), latest_timestamp.date(), sensors=['1371', '1368', '1377'], output_path=plot_output_path)
                
                logger.info(f"Processed {file} and saved results in {output_folder}")
                
def process_single_dat_file(dat_file_path, output_folder):
    file_name = os.path.basename(dat_file_path)
    output_csv_1368_path = os.path.join(output_folder, f"CanopyTemp_1368_{os.path.splitext(file_name)[0]}.csv")
    
    logger.info(f"Processing file: {dat_file_path}")
    processed_data_df = parse_dat_file_to_csv_with_temps(dat_file_path, output_csv_1368_path)
    
    # Plot canopy temperatures for the last 10 days for all sensors
    latest_timestamp = processed_data_df["TIMESTAMP"].max()
    start_date_last_10_days = latest_timestamp - pd.Timedelta(days=10)
    filtered_data_last_10_days = processed_data_df[
        (processed_data_df["TIMESTAMP"] >= start_date_last_10_days) &
        (processed_data_df["TIMESTAMP"] <= latest_timestamp)
    ]
    plot_output_path = os.path.join(output_folder, f"CanopyTemp_plot_{os.path.splitext(file_name)[0]}.png")
    plot_canopy_temperatures(filtered_data_last_10_days, start_date_last_10_days.date(), latest_timestamp.date(), sensors=['1371', '1368', '1377'], output_path=plot_output_path)
    
    logger.info(f"Processed {file_name} and saved results in {output_folder}")

def main(input_folders, output_folder):
    os.makedirs(output_folder, exist_ok=True)
    for folder in input_folders:
        process_dat_files(folder, output_folder)

if __name__ == "__main__":
    corn_base_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Projects\Students\Bryan Nsoh\Data\2024_data_corn_lnr"
    soybean_base_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Projects\Students\Bryan Nsoh\Data\Soybean Lnr"
    output_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\data"
    main([corn_base_folder, soybean_base_folder], output_folder)
        </content>
    </file>
    <file>
        <name>dat_to_csv.py</name>
        <path>src\dat_to_csv.py</path>
        <content>
import os
import re
import pandas as pd
import numpy as np
import yaml
from datetime import datetime, timedelta

def load_sensor_mapping(file_path):
    with open(file_path, 'r') as file:
        return yaml.safe_load(file)

def parse_dat_file(file_name):
    with open(file_name, "r") as file:
        lines = file.readlines()
    headers = lines[1].strip().split(",")
    data_lines = lines[4:]
    data = pd.DataFrame([line.strip().split(",") for line in data_lines], columns=headers)
    
    data.columns = data.columns.str.replace('"', "").str.replace("RECORD", "RecNbr")
    data.columns = data.columns.str.replace("_Avg", "")
    data = data.replace({"NAN": np.nan, '"NAN"': np.nan})
    
    # Use a copy to avoid the FutureWarning
    data = data.replace({"NAN": np.nan, '"NAN"': np.nan}).copy()
    data["TIMESTAMP"] = data["TIMESTAMP"].str.replace('"', "")
    
    for col in data.columns:
        if col != "TIMESTAMP":
            data[col] = pd.to_numeric(data[col], errors='coerce')
    
    data["TIMESTAMP"] = pd.to_datetime(data["TIMESTAMP"], errors="coerce")
    data = data[~data["TIMESTAMP"].isna()]
    
    # Correct column names if necessary
    if 'TDR5006B11724' in data.columns:
        data['TDR5006B11824'] = data['TDR5006B11724']
        data.drop('TDR5006B11724', axis=1, inplace=True)
    
    if 'TDR5026A23824' in data.columns:
        data['TDR5026A23024'] = data['TDR5026A23824']
        data.drop('TDR5026A23824', axis=1, inplace=True)
    
    # Resample to hourly data
    data_hourly = data.set_index("TIMESTAMP").resample('h').mean().reset_index()
    
    return data_hourly.sort_values("TIMESTAMP")  # Sort by timestamp

def get_dat_files(folder_path, crop_type):
    if crop_type == 'corn':
        patterns = [r'nodeA.*\.dat', r'nodeB.*\.dat', r'nodeC.*\.dat']
    elif crop_type == 'soybean':
        patterns = [r'SoyNodeA.*_NodeA\.dat', r'SoyNodeB.*_NodeB\.dat', r'SoyNodeC.*_NodeC\.dat']
    
    dat_files = []
    for file in os.listdir(folder_path):
        for pattern in patterns:
            if re.match(pattern, file, re.IGNORECASE):
                dat_files.append(os.path.join(folder_path, file))
                break
    return dat_files

def parse_weather_csv(filename):
    df = pd.read_csv(
        filename,
        header=1,
        skiprows=[2, 3],
        parse_dates=["TIMESTAMP"],
        date_format="%Y-%m-%d %H:%M:%S",
        low_memory=False
    )
    
    df = df.rename(columns=lambda x: x.strip())
    df["TIMESTAMP"] = pd.to_datetime(df["TIMESTAMP"], errors="coerce")
    df = df.dropna(subset=["TIMESTAMP"])
    df = df.apply(pd.to_numeric, errors="coerce")

    # Select only the required weather columns
    weather_columns = ['TIMESTAMP', 'Ta_2m_Avg', 'RH_2m_Avg', 'WndAveSpd_3m', 'WndAveDir_3m', 'PresAvg_1pnt5m', 'Solar_2m_Avg', 'Rain_1m_Tot', 'TaMax_2m', 'TaMin_2m', 'RHMax_2m', 'RHMin_2m']
    df = df[weather_columns]

    # Ensure TIMESTAMP is recognized as datetime
    df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'])

    # Shift timestamps back by 5 hours
    df['TIMESTAMP'] = df['TIMESTAMP'] - timedelta(hours=5)

    # Resample to hourly data
    df_hourly = df.set_index("TIMESTAMP").resample('h').mean().reset_index()
    
    return df_hourly.sort_values("TIMESTAMP")  # Sort by timestamp

def process_folder(folder_path, sensor_mapping, crop_type, output_folder, weather_data):
    dat_files = get_dat_files(folder_path, crop_type)
    
    for dat_file in dat_files:
        if os.path.exists(dat_file):
            print(f"Processing file: {dat_file}")
            df = parse_dat_file(dat_file)
            crop_specific_mapping = [sensor for sensor in sensor_mapping if sensor['field'] == f'LINEAR_{crop_type.upper()}']
            process_and_save_data(df, crop_specific_mapping, crop_type, output_folder, weather_data)
        else:
            print(f"File not found: {dat_file}")

def process_and_save_data(df, sensor_mapping, crop_type, output_folder, weather_data):
    sensor_groups = {}
    for sensor in sensor_mapping:
        key = (sensor['treatment'], sensor['plot_number'], sensor['field'])
        if key not in sensor_groups:
            sensor_groups[key] = []
        sensor_groups[key].append(sensor['sensor_id'])

    for (treatment, plot_number, field), sensors in sensor_groups.items():
        columns_to_save = ['TIMESTAMP'] + [s for s in sensors if s in df.columns]
        df_to_save = df[columns_to_save].dropna(subset=columns_to_save[1:], how='all')
        
        if not df_to_save.empty:
            # Get the date range where at least one sensor has data
            start_date = df_to_save['TIMESTAMP'].min()
            end_date = df_to_save['TIMESTAMP'].max()

            # Filter weather data to match the sensor data range
            weather_data_filtered = weather_data[
                (weather_data['TIMESTAMP'] >= start_date) &
                (weather_data['TIMESTAMP'] <= end_date)
            ]

            # Merge with filtered weather data
            merged_df = pd.merge(df_to_save, weather_data_filtered, on='TIMESTAMP', how='outer')
            merged_df = merged_df.sort_values("TIMESTAMP")

            file_name = f"{field}_trt{treatment}_plot_{plot_number}_{datetime.now().strftime('%Y%m%d')}.csv"
            output_path = os.path.join(output_folder, file_name)
            merged_df.to_csv(output_path, index=False)
            print(f"Saved data to {output_path}")
        else:
            print(f"No data to save for {field} plot {plot_number}")

def create_dated_folder(base_path):
    current_date = datetime.now().strftime("%Y-%m-%d")
    dated_folder = os.path.join(base_path, f"data-{current_date}")
    os.makedirs(dated_folder, exist_ok=True)
    return dated_folder

def main(corn_folders, soybean_folders, sensor_mapping_path, output_folder, weather_csv_path):
    sensor_mapping = load_sensor_mapping(sensor_mapping_path)
    
    dated_output_folder = create_dated_folder(output_folder)
    
    # Load and process weather data
    weather_data = parse_weather_csv(weather_csv_path)
    
    print("Processing corn data")
    for folder in corn_folders:
        print(f"Processing corn folder: {folder}")
        process_folder(folder, sensor_mapping, crop_type='corn', output_folder=dated_output_folder, weather_data=weather_data)
    
    print("Processing soybean data")
    for folder in soybean_folders:
        print(f"Processing soybean folder: {folder}")
        process_folder(folder, sensor_mapping, crop_type='soybean', output_folder=dated_output_folder, weather_data=weather_data)

if __name__ == "__main__":
    corn_folders = [
        r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Projects\Students\Bryan Nsoh\Data\2024_data_corn_lnr\07-03-2024",
        r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Projects\Students\Bryan Nsoh\Data\2024_data_corn_lnr\07-08-2024-discontinuous",
        r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Projects\Students\Bryan Nsoh\Data\2024_data_corn_lnr\07-14-2024-discont-nodeC only",
        r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Projects\Students\Bryan Nsoh\Data\2024_data_corn_lnr\07-15-2024-discont-unsure",
        r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Projects\Students\Bryan Nsoh\Data\2024_data_corn_lnr\07-19-2024"
    ]
    soybean_folders = [
        r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Projects\Students\Bryan Nsoh\Data\Soybean Lnr\07-15-24",
        r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Projects\Students\Bryan Nsoh\Data\Soybean Lnr\07-19-2024"
    ]
    sensor_mapping_path = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\sensor_mapping.yaml"
    output_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\data"
    weather_csv_path = r"C:\Users\bnsoh2\Downloads\North_Platte_3SW_Beta_1min (8).csv"
    
    main(corn_folders, soybean_folders, sensor_mapping_path, output_folder, weather_csv_path)
        </content>
    </file>
    <file>
        <name>et.py</name>
        <path>src\et.py</path>
        <content>
# src/et.py

import os
import pandas as pd
import numpy as np
import pyet
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# User-defined current crop stage
CURRENT_CROP_STAGE = 'mid-season'  # Options: 'initial', 'development', 'mid-season', 'late-season'

# Metadata
ELEVATION = 876  # meters
LATITUDE = 41.15  # degrees
LONGITUDE = -100.77  # degrees
WIND_HEIGHT = 3  # meters
STEFAN_BOLTZMANN = 5.67e-8
CP = 1005
GRAVITY = 9.81
K = 0.41
CROP_HEIGHT = 1.6
SURFACE_ALBEDO = 0.23

EMERGENCE_DATE = '2023-05-15'  # Format: 'YYYY-MM-DD'

# Crop coefficients and growth stages (days)
CROP_DATA = {
    'corn': {
        'stages': [30, 40, 50, 30],  # [initial, development, mid-season, late-season]
        'kc': [0.3, 1.2, 0.35]  # [kc_ini, kc_mid, kc_end]
    },
    'soybean': {
        'stages': [15, 30, 55, 25],
        'kc': [0.4, 1.15, 0.5]
    }
}

def get_crop_coefficient(crop_type):
    kc_values = CROP_DATA[crop_type]['kc']
    
    if CURRENT_CROP_STAGE == 'initial':
        return kc_values[0]
    elif CURRENT_CROP_STAGE == 'development':
        return np.mean([kc_values[0], kc_values[1]])
    elif CURRENT_CROP_STAGE == 'mid-season':
        return kc_values[1]
    elif CURRENT_CROP_STAGE == 'late-season':
        return np.mean([kc_values[1], kc_values[2]])
    else:
        logger.warning(f"Invalid crop stage: {CURRENT_CROP_STAGE}. Using mid-season Kc.")
        return kc_values[1]

def get_crop_type_from_filename(file_path):
    file_name = os.path.basename(file_path)
    if 'CORN' in file_name.upper():
        return 'corn'
    elif 'SOYBEAN' in file_name.upper():
        return 'soybean'
    else:
        logger.warning(f"Unable to determine crop type from filename: {file_name}")
        return None

def calculate_et(df, crop_type):
    # Resample to daily, handling missing data
    df_daily = df.set_index('TIMESTAMP').resample('D').mean()
    
    required_columns = ['Ta_2m_Avg', 'TaMax_2m', 'TaMin_2m', 'RHMax_2m', 'RHMin_2m', 'WndAveSpd_3m', 'Solar_2m_Avg']
    df_daily = df_daily.dropna(subset=required_columns)
    
    if df_daily.empty:
        logger.warning(f"Dataframe is empty after resampling and dropping NaN values for {crop_type}.")
        return pd.DataFrame(columns=['TIMESTAMP', 'eto', 'etc'])
    
    # Convert solar radiation to MJ/m^2/day
    df_daily['Solar_2m_Avg_MJ'] = df_daily['Solar_2m_Avg'] * 0.0864
    
    lat_rad = LATITUDE * np.pi / 180
    
    # Prepare input data
    inputs = {
        'tmean': df_daily['Ta_2m_Avg'],
        'wind': df_daily['WndAveSpd_3m'],
        'rs': df_daily['Solar_2m_Avg_MJ'],
        'tmax': df_daily['TaMax_2m'],
        'tmin': df_daily['TaMin_2m'],
        'rh': (df_daily['RHMax_2m'] + df_daily['RHMin_2m']) / 2,
        'elevation': ELEVATION,
        'lat': lat_rad
    }
    
    # Calculate ETo
    df_daily['eto'] = pyet.combination.pm_asce(**inputs)
    
    # Get Kc based on current crop stage
    kc = get_crop_coefficient(crop_type)
    
    # Calculate ETc
    df_daily['etc'] = df_daily['eto'] * kc
    df_daily['kc'] = kc
    
    # Reset index to get TIMESTAMP as a column and return required columns
    return df_daily.reset_index()[['TIMESTAMP', 'eto', 'etc', 'kc']]

def process_csv_file(file_path):
    logger.info(f"Processing file for ET calculation: {file_path}")
    
    # Determine crop type from file name
    crop_type = get_crop_type_from_filename(file_path)
    if crop_type is None:
        logger.error(f"Skipping file due to unknown crop type: {file_path}")
        return None
    
    df = pd.read_csv(file_path, parse_dates=['TIMESTAMP'])
    
    et_df = calculate_et(df, crop_type)
    
    if not et_df.empty:
        # Merge ET data back into original dataframe
        df = pd.merge(df, et_df, on='TIMESTAMP', how='left')
        
        # Log ET statistics
        for et_type in ['eto', 'etc']:
            valid_et = df[et_type].dropna()
            if len(valid_et) > 0:
                logger.info(f"{et_type.upper()} stats for {crop_type} - Count: {len(valid_et)}, Mean: {valid_et.mean():.4f}, Min: {valid_et.min():.4f}, Max: {valid_et.max():.4f}")
            else:
                logger.info(f"No valid {et_type.upper()} values calculated for {crop_type}")
        
        # Save updated dataframe
        df.to_csv(file_path, index=False)
        logger.info(f"Updated ET values in file: {file_path}")
    else:
        logger.warning(f"No ET values calculated for file: {file_path}")
    
    return df

def main(input_folder):
    for root, _, files in os.walk(input_folder):
        for file in files:
            if file.endswith('.csv'):
                file_path = os.path.join(root, file)
                process_csv_file(file_path)

if __name__ == "__main__":
    input_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\data"
    main(input_folder)
        </content>
    </file>
    <file>
        <name>fuzz.py</name>
        <path>src\fuzz.py</path>
        <content>
# src/fuzz.py

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from skfuzzy import control as ctrl
import skfuzzy as fuzz
from datetime import datetime, timedelta
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FuzzyIrrigationController:
    def __init__(self):
        self.setup_fuzzy_system()

    def setup_fuzzy_system(self):
        # Define input ranges
        x_et = np.arange(0, 20, 0.1)
        x_swsi = np.arange(0, 1, 0.01)
        x_cwsi = np.arange(0, 1, 0.01)
        x_irrigation = np.arange(0, 1, 0.01)
    
        # Create fuzzy variables
        self.etc = ctrl.Antecedent(x_et, 'etc')
        self.swsi = ctrl.Antecedent(x_swsi, 'swsi')
        self.cwsi = ctrl.Antecedent(x_cwsi, 'cwsi')
        self.irrigation = ctrl.Consequent(x_irrigation, 'irrigation')
    
        # Define membership functions for ET
        self.etc['very_low'] = fuzz.trimf(x_et, [0, 0, 2])
        self.etc['low'] = fuzz.trimf(x_et, [1, 2.5, 4])
        self.etc['medium'] = fuzz.trimf(x_et, [3, 4.5, 6])
        self.etc['high'] = fuzz.trimf(x_et, [5, 6.5, 8])
        self.etc['very_high'] = fuzz.trimf(x_et, [7, 10, 10])
    
        self.swsi['very_wet'] = fuzz.trimf(x_swsi, [0, 0, 0.25])
        self.swsi['wet'] = fuzz.trimf(x_swsi, [0, 0.25, 0.5])
        self.swsi['normal'] = fuzz.trimf(x_swsi, [0.25, 0.5, 0.75])
        self.swsi['dry'] = fuzz.trimf(x_swsi, [0.5, 0.75, 1])
        self.swsi['very_dry'] = fuzz.trimf(x_swsi, [0.75, 1, 1])
    
        self.cwsi['no_stress'] = fuzz.trimf(x_cwsi, [0, 0, 0.25])
        self.cwsi['low_stress'] = fuzz.trimf(x_cwsi, [0, 0.25, 0.5])
        self.cwsi['moderate_stress'] = fuzz.trimf(x_cwsi, [0.25, 0.5, 0.75])
        self.cwsi['high_stress'] = fuzz.trimf(x_cwsi, [0.5, 0.75, 1])
        self.cwsi['severe_stress'] = fuzz.trimf(x_cwsi, [0.75, 1, 1])
    
        self.irrigation['none'] = fuzz.trimf(x_irrigation, [0, 0, 0.2])
        self.irrigation['very_low'] = fuzz.trimf(x_irrigation, [0, 0.2, 0.4])
        self.irrigation['low'] = fuzz.trimf(x_irrigation, [0.2, 0.4, 0.6])
        self.irrigation['medium'] = fuzz.trimf(x_irrigation, [0.4, 0.6, 0.8])
        self.irrigation['high'] = fuzz.trimf(x_irrigation, [0.6, 0.8, 1])
        self.irrigation['very_high'] = fuzz.trimf(x_irrigation, [0.8, 1, 1])
    
        # Define fuzzy rules
        rules = [
            ctrl.Rule(self.cwsi['severe_stress'], self.irrigation['very_high']),
            ctrl.Rule(self.cwsi['high_stress'], self.irrigation['high']),
            ctrl.Rule(self.cwsi['moderate_stress'], self.irrigation['medium']),
            ctrl.Rule(self.cwsi['low_stress'], self.irrigation['low']),
            ctrl.Rule(self.cwsi['no_stress'], self.irrigation['very_low']),
            
            ctrl.Rule(self.swsi['very_dry'], self.irrigation['very_high']),
            ctrl.Rule(self.swsi['dry'], self.irrigation['high']),
            ctrl.Rule(self.swsi['normal'], self.irrigation['medium']),
            ctrl.Rule(self.swsi['wet'], self.irrigation['low']),
            ctrl.Rule(self.swsi['very_wet'], self.irrigation['none']),
            
            ctrl.Rule(self.cwsi['severe_stress'] & self.swsi['very_dry'], self.irrigation['very_high']),
            ctrl.Rule(self.cwsi['high_stress'] & self.swsi['dry'], self.irrigation['high']),
            ctrl.Rule(self.cwsi['moderate_stress'] & self.swsi['normal'], self.irrigation['medium']),
            ctrl.Rule(self.cwsi['low_stress'] & self.swsi['wet'], self.irrigation['low']),
            ctrl.Rule(self.cwsi['no_stress'] & self.swsi['very_wet'], self.irrigation['none']),
            
            ctrl.Rule(self.etc['very_high'] & self.cwsi['high_stress'], self.irrigation['high']),
            ctrl.Rule(self.etc['high'] & self.swsi['dry'], self.irrigation['medium']),
            ctrl.Rule(self.etc['medium'] & (self.cwsi['moderate_stress'] | self.swsi['normal']), self.irrigation['medium']),
            ctrl.Rule(self.etc['low'] & (self.cwsi['low_stress'] | self.swsi['wet']), self.irrigation['low']),
            ctrl.Rule(self.etc['very_low'] & (self.cwsi['no_stress'] | self.swsi['very_wet']), self.irrigation['none']),
        ]
    
        # Create and simulate control system
        self.irrigation_ctrl = ctrl.ControlSystem(rules)
        self.irrigation_sim = ctrl.ControlSystemSimulation(self.irrigation_ctrl)

    def get_recent_swsi(self, series):
        # Get the most recent non-NaN SWSI value
        return series.dropna().iloc[-1] if not series.dropna().empty else None

    def get_recent_cwsi(self, series, n_days=3):
        end_date = pd.Timestamp.now().floor('D')
        start_date = end_date - pd.Timedelta(days=n_days)
        
        # Ensure the series index is timezone-naive
        if series.index.tz is not None:
            series.index = series.index.tz_localize(None)
        
        recent_data = series.loc[start_date:end_date]
        valid_data = recent_data[(recent_data >= 0) & (recent_data <= 1.5)]
        
        return valid_data.iloc[-1] if not valid_data.empty else None

    def compute_irrigation(self, df, plot):
        # Compute inputs for fuzzy system
        et_data, et_start, et_end = self.get_recent_values(df['etc'], 6)
        et_avg = et_data.mean()

        swsi_value = self.get_recent_swsi(df['swsi'])
        cwsi_value = self.get_recent_cwsi(df['cwsi'])

        # Log the values used for decision making
        logger.info(f"Values used for irrigation decision for plot {plot}:")
        logger.info(f"ET Average: {et_avg:.2f} (from {et_start} to {et_end})")
        logger.info(f"Most recent SWSI: {swsi_value:.2f}" if swsi_value is not None else "No valid SWSI value found")
        logger.info(f"Most recent valid CWSI: {cwsi_value:.2f}" if cwsi_value is not None else "No valid CWSI value found in the last 3 days")

        # Set inputs for fuzzy system
        self.irrigation_sim.input['etc'] = et_avg
        self.irrigation_sim.input['swsi'] = swsi_value if swsi_value is not None else 0.5  # Default value if no valid SWSI
        self.irrigation_sim.input['cwsi'] = min(cwsi_value, 1) if cwsi_value is not None else 0.5  # Default value if no valid CWSI, cap at 1

        # Compute output
        self.irrigation_sim.compute()

        irrigation_amount = self.irrigation_sim.output['irrigation']

        logger.info(f"Recommended Irrigation Amount for plot {plot}: {irrigation_amount:.2f} inches")

        return irrigation_amount, et_avg, swsi_value, cwsi_value

    def get_recent_values(self, series, n_days=3):
        end_date = pd.Timestamp.now().floor('D')
        start_date = end_date - pd.Timedelta(days=n_days)
        
        # Ensure the series index is timezone-naive
        if series.index.tz is not None:
            series.index = series.index.tz_localize(None)
        
        recent_data = series.loc[start_date:end_date]
        return recent_data, start_date, end_date

def process_csv_file(file_path, controller):
    logger.info(f"Processing file for irrigation recommendation: {file_path}")
    
    df = pd.read_csv(file_path, parse_dates=['TIMESTAMP'])
    df.set_index('TIMESTAMP', inplace=True)
    
    plot_number = file_path.split('_')[-2]  # Assuming the plot number is the second-to-last part of the filename
    crop_type = 'corn' if 'CORN' in file_path else 'soybean'

    irrigation_amount, et_avg, swsi_value, cwsi_value = controller.compute_irrigation(df, plot_number)
    
    return {
        'plot': plot_number,
        'crop': crop_type,
        'irrigation': irrigation_amount,
        'et_avg': et_avg,
        'swsi': swsi_value,
        'cwsi': cwsi_value
    }

def main(input_folder, output_file):
    controller = FuzzyIrrigationController()
    recommendations = []

    for root, _, files in os.walk(input_folder):
        for file in files:
            if file.endswith('.csv') and ('CORN' in file or 'SOYBEAN' in file) and 'trt1' in file:
                file_path = os.path.join(root, file)
                result = process_csv_file(file_path, controller)
                recommendations.append(result)

    # Create recommendations DataFrame and save to CSV
    recommendations_df = pd.DataFrame(recommendations)
    recommendations_df.to_csv(output_file, index=False)
    logger.info(f"Recommendations saved to {output_file}")

if __name__ == "__main__":
    input_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\data"
    output_file = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\recommendations\fuzzy-trt-1.csv"
    main(input_folder, output_file)
        </content>
    </file>
    <file>
        <name>fuzz_with_visuals.py</name>
        <path>src\fuzz_with_visuals.py</path>
        <content>
# src/fuzz_with_visuals.py

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from skfuzzy import control as ctrl
import skfuzzy as fuzz
from datetime import datetime, timedelta
import logging
from matplotlib import gridspec

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FuzzyIrrigationController:
    def __init__(self, days_back=3):
        self.days_back = days_back
        self.setup_fuzzy_system()
        self.image_dir = os.path.join(os.getcwd(), 'images')
        os.makedirs(self.image_dir, exist_ok=True)

    def setup_fuzzy_system(self):
        # Define input ranges
        x_et = np.arange(0, 20, 0.1)
        x_swsi = np.arange(0, 1, 0.01)
        x_cwsi = np.arange(0, 1, 0.01)
        x_irrigation = np.arange(0, 1, 0.01)
    
        # Create fuzzy variables
        self.etc = ctrl.Antecedent(x_et, 'etc')
        self.swsi = ctrl.Antecedent(x_swsi, 'swsi')
        self.cwsi = ctrl.Antecedent(x_cwsi, 'cwsi')
        self.irrigation = ctrl.Consequent(x_irrigation, 'irrigation')
    
        # Define membership functions for ET
        self.etc['very_low'] = fuzz.trimf(x_et, [0, 0, 2])
        self.etc['low'] = fuzz.trimf(x_et, [1, 2.5, 4])
        self.etc['medium'] = fuzz.trimf(x_et, [3, 4.5, 6])
        self.etc['high'] = fuzz.trimf(x_et, [5, 6.5, 8])
        self.etc['very_high'] = fuzz.trimf(x_et, [7, 10, 10])
    
        self.swsi['very_wet'] = fuzz.trimf(x_swsi, [0, 0, 0.25])
        self.swsi['wet'] = fuzz.trimf(x_swsi, [0, 0.25, 0.5])
        self.swsi['normal'] = fuzz.trimf(x_swsi, [0.25, 0.5, 0.75])
        self.swsi['dry'] = fuzz.trimf(x_swsi, [0.5, 0.75, 1])
        self.swsi['very_dry'] = fuzz.trimf(x_swsi, [0.75, 1, 1])
    
        self.cwsi['no_stress'] = fuzz.trimf(x_cwsi, [0, 0, 0.25])
        self.cwsi['low_stress'] = fuzz.trimf(x_cwsi, [0, 0.25, 0.5])
        self.cwsi['moderate_stress'] = fuzz.trimf(x_cwsi, [0.25, 0.5, 0.75])
        self.cwsi['high_stress'] = fuzz.trimf(x_cwsi, [0.5, 0.75, 1])
        self.cwsi['severe_stress'] = fuzz.trimf(x_cwsi, [0.75, 1, 1])
    
        self.irrigation['none'] = fuzz.trimf(x_irrigation, [0, 0, 0.2])
        self.irrigation['very_low'] = fuzz.trimf(x_irrigation, [0, 0.2, 0.4])
        self.irrigation['low'] = fuzz.trimf(x_irrigation, [0.2, 0.4, 0.6])
        self.irrigation['medium'] = fuzz.trimf(x_irrigation, [0.4, 0.6, 0.8])
        self.irrigation['high'] = fuzz.trimf(x_irrigation, [0.6, 0.8, 1])
        self.irrigation['very_high'] = fuzz.trimf(x_irrigation, [0.8, 1, 1])
    
        rules = [
            # Strong rule to disregard CWSI when soil is very wet
            ctrl.Rule(self.swsi['very_wet'], self.irrigation['none']),
            
            # Existing CWSI rules, now only apply when soil is not very wet
            ctrl.Rule(self.cwsi['severe_stress'] & ~self.swsi['very_wet'], self.irrigation['very_high']),
            ctrl.Rule(self.cwsi['high_stress'] & ~self.swsi['very_wet'], self.irrigation['high']),
            ctrl.Rule(self.cwsi['moderate_stress'] & ~self.swsi['very_wet'], self.irrigation['medium']),
            ctrl.Rule(self.cwsi['low_stress'] & ~self.swsi['very_wet'], self.irrigation['low']),
            ctrl.Rule(self.cwsi['no_stress'] & ~self.swsi['very_wet'], self.irrigation['very_low']),
            
            # Existing SWSI rules (unchanged)
            ctrl.Rule(self.swsi['very_dry'], self.irrigation['very_high']),
            ctrl.Rule(self.swsi['dry'], self.irrigation['high']),
            ctrl.Rule(self.swsi['normal'], self.irrigation['medium']),
            ctrl.Rule(self.swsi['wet'], self.irrigation['low']),
            
            # Existing combined CWSI and SWSI rules, now only apply when soil is not very wet
            ctrl.Rule(self.cwsi['severe_stress'] & self.swsi['very_dry'] & ~self.swsi['very_wet'], self.irrigation['very_high']),
            ctrl.Rule(self.cwsi['high_stress'] & self.swsi['dry'] & ~self.swsi['very_wet'], self.irrigation['high']),
            ctrl.Rule(self.cwsi['moderate_stress'] & self.swsi['normal'] & ~self.swsi['very_wet'], self.irrigation['medium']),
            ctrl.Rule(self.cwsi['low_stress'] & self.swsi['wet'] & ~self.swsi['very_wet'], self.irrigation['low']),
            
            # Existing ET rules, modified to not apply CWSI when soil is very wet
            ctrl.Rule(self.etc['very_high'] & self.cwsi['high_stress'] & ~self.swsi['very_wet'], self.irrigation['high']),
            ctrl.Rule(self.etc['high'] & self.swsi['dry'], self.irrigation['medium']),
            ctrl.Rule(self.etc['medium'] & ((self.cwsi['moderate_stress'] & ~self.swsi['very_wet']) | self.swsi['normal']), self.irrigation['medium']),
            ctrl.Rule(self.etc['low'] & ((self.cwsi['low_stress'] & ~self.swsi['very_wet']) | self.swsi['wet']), self.irrigation['low']),
            ctrl.Rule(self.etc['very_low'] & (self.cwsi['no_stress'] | self.swsi['very_wet']), self.irrigation['none']),
        ]
    
        # Create and simulate control system
        self.irrigation_ctrl = ctrl.ControlSystem(rules)
        self.irrigation_sim = ctrl.ControlSystemSimulation(self.irrigation_ctrl)

    def plot_membership_functions(self):
        fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows=4, figsize=(10, 20))
        
        self.etc.view(ax=ax0)
        ax0.set_title('Evapotranspiration')
        self.swsi.view(ax=ax1)
        ax1.set_title('Surface Water Supply Index')
        self.cwsi.view(ax=ax2)
        ax2.set_title('Crop Water Stress Index')
        self.irrigation.view(ax=ax3)
        ax3.set_title('Irrigation')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.image_dir, 'membership_functions.png'))
        plt.close()

    def plot_recent_data(self, et_data, swsi_data, cwsi_data, plot):
        fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, figsize=(10, 15))
        
        et_data.plot(ax=ax1)
        ax1.set_title('Recent Evapotranspiration')
        ax1.set_ylabel('ET')
        
        swsi_data.plot(ax=ax2)
        ax2.set_title('Recent Surface Water Supply Index')
        ax2.set_ylabel('SWSI')
        
        cwsi_data.plot(ax=ax3)
        ax3.set_title('Recent Crop Water Stress Index')
        ax3.set_ylabel('CWSI')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.image_dir, f'recent_data_plot_{plot}.png'))
        plt.close()

    def plot_fuzzy_output(self, et_avg, swsi_avg, cwsi_max, irrigation_amount, plot):
        fig, ax = plt.subplots(figsize=(8, 6))
        
        self.irrigation.view(sim=self.irrigation_sim, ax=ax)
        ax.plot([irrigation_amount, irrigation_amount], [0, 1], 'r--', linewidth=1.5, label='Output')
        
        ax.set_title(f'Fuzzy Irrigation Output (Plot {plot})')
        ax.set_ylabel('Membership')
        ax.set_xlabel('Irrigation Amount')
        ax.legend()
        
        plt.text(0.05, 0.95, f'Inputs:\nET: {et_avg:.2f}\nSWSI: {swsi_avg:.2f}\nCWSI: {cwsi_max:.2f}', 
                 transform=ax.transAxes, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.image_dir, f'fuzzy_output_plot_{plot}.png'))
        plt.close()

    def get_recent_values(self, series):
        end_date = pd.Timestamp.now().floor('D')
        start_date = end_date - pd.Timedelta(days=self.days_back)
        
        # Ensure the series index is timezone-naive
        if series.index.tz is not None:
            series.index = series.index.tz_localize(None)
        
        recent_data = series.loc[start_date:end_date]
        return recent_data, start_date, end_date

    def get_recent_swsi(self, series):
        # Get the most recent non-NaN SWSI value
        return series.dropna().iloc[-1] if not series.dropna().empty else None

    def get_recent_cwsi(self, series):
        end_date = pd.Timestamp.now().floor('D')
        start_date = end_date - pd.Timedelta(days=self.days_back)
        
        # Ensure the series index is timezone-naive
        if series.index.tz is not None:
            series.index = series.index.tz_localize(None)
        
        # Get data for the last self.days_back days
        recent_data = series.loc[start_date:end_date]
        
        # Group by date and get the maximum CWSI for each day
        daily_max = recent_data.groupby(recent_data.index.date).max()
        
        # Filter valid CWSI values (between 0 and 1.5)
        valid_data = daily_max[(daily_max >= 0) & (daily_max <= 1.5)]
        
        if len(valid_data) > 0:
            # Sort values in descending order and take the average of up to 3 highest values
            top_3_avg = valid_data.sort_values(ascending=False).head(3).mean()
            # Cap the average at 1  
            return min(top_3_avg, 1)
        else:
            return None

    def compute_irrigation(self, df, plot):
        recommendations = []
        
        for _, row in df.iterrows():
            et_value = row['etc']
            swsi_value = row['swsi']
            cwsi_value = row['cwsi']
            
            # Set inputs for fuzzy system
            self.irrigation_sim.input['etc'] = et_value if not pd.isna(et_value) else 0
            self.irrigation_sim.input['swsi'] = swsi_value if not pd.isna(swsi_value) else 0.5
            self.irrigation_sim.input['cwsi'] = min(cwsi_value, 1) if not pd.isna(cwsi_value) else 0.5
            
            # Compute output
            self.irrigation_sim.compute()
            
            irrigation_amount = self.irrigation_sim.output['irrigation']
            recommendations.append(irrigation_amount)
        
        return recommendations

    def process_csv_file(self, file_path):
        logger.info(f"Processing file for irrigation recommendation: {file_path}")
        
        df = pd.read_csv(file_path, parse_dates=['TIMESTAMP'])
        df.set_index('TIMESTAMP', inplace=True)
        
        plot_number = file_path.split('_')[-2]  # Assuming the plot number is the second-to-last part of the filename
        crop_type = 'corn' if 'CORN' in file_path else 'soybean'

        recommendations = self.compute_irrigation(df, plot_number)
        
        # Add recommendations to the main data CSV
        df['recommendation'] = recommendations
        df.to_csv(file_path)  # Overwrite the original CSV with the new column
        
        # For the summary, we'll use today's values or the last available if today's not present
        today = pd.Timestamp.now().floor('D')
        if today in df.index:
            summary_row = df.loc[today]
        else:
            summary_row = df.iloc[-1]
        
        return {
            'plot': plot_number,
            'crop': crop_type,
            'irrigation': summary_row['recommendation'],
            'et_avg': summary_row['etc'],
            'swsi': summary_row['swsi'],
            'cwsi': summary_row['cwsi'],
            'date': summary_row.name.strftime('%Y-%m-%d')
        }

def main(input_folder, output_file, days_back=4):
    controller = FuzzyIrrigationController(days_back)
    
    # Plot membership functions
    controller.plot_membership_functions()
    
    recommendations = []

    for root, _, files in os.walk(input_folder):
        for file in files:
            if file.endswith('.csv') and ('CORN' in file or 'SOYBEAN' in file) and 'trt1' in file:
                file_path = os.path.join(root, file)
                result = controller.process_csv_file(file_path)
                recommendations.append(result)

    # Create recommendations DataFrame and save to CSV
    recommendations_df = pd.DataFrame(recommendations)
    recommendations_df.to_csv(output_file, index=False)
    logger.info(f"Recommendations saved to {output_file}")

    # Visualize the fuzzy control system and rule base
    controller.irrigation_ctrl.view()
    plt.savefig(os.path.join(controller.image_dir, 'fuzzy_control_system.png'))
    plt.close()

if __name__ == "__main__":
    input_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\data"
    output_file = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\recommendations\fuzzy-trt-1.csv"
    main(input_folder, output_file, days_back=6)
        </content>
    </file>
    <file>
        <name>get_forecasts.py</name>
        <path>src\get_forecasts.py</path>
        <content>
import os
import requests
import pandas as pd
import numpy as np
import pyet
import logging
from datetime import datetime
import pytz
from dotenv import load_dotenv

# Load API key from .env file
load_dotenv()
API_KEY = os.getenv('OPENWEATHERMAP_API_KEY')

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# OpenWeatherMap API details
BASE_URL = "https://api.openweathermap.org/data/2.5/forecast"

# Metadata for ET calculation
ELEVATION = 876  # meters
LATITUDE = 41.15  # degrees
LONGITUDE = -100.77  # degrees
WIND_HEIGHT = 3  # meters

def get_forecast(lat, lon):
    params = {
        'lat': lat,
        'lon': lon,
        'appid': API_KEY,
        'units': 'metric'
    }
    logger.info(f"Requesting forecast data for coordinates: {lat}, {lon}")
    response = requests.get(BASE_URL, params=params)
    response.raise_for_status()
    logger.info("Successfully retrieved forecast data")
    return response.json()

def get_solar_radiation_forecast(lat, lon):
    solar_url = f"https://api.openweathermap.org/data/2.5/solar_radiation/forecast"
    params = {
        'lat': lat,
        'lon': lon,
        'appid': API_KEY
    }
    logger.info(f"Requesting solar radiation forecast data for coordinates: {lat}, {lon}")
    response = requests.get(solar_url, params=params)
    response.raise_for_status()
    logger.info("Successfully retrieved solar radiation forecast data")
    return response.json()

def map_forecast_data(forecast_item, collection_time):
    return {
        'collection_time': collection_time.isoformat(),
        'TIMESTAMP': datetime.utcfromtimestamp(forecast_item['dt']).isoformat(),
        'Ta_2m_Avg': forecast_item['main']['temp'],
        'TaMax_2m': forecast_item['main']['temp_max'],
        'TaMin_2m': forecast_item['main']['temp_min'],
        'RH_2m_Avg': forecast_item['main']['humidity'],
        'Dp_2m_Avg': forecast_item['main'].get('dew_point'),
        'WndAveSpd_3m': forecast_item['wind']['speed'],
        'WndAveDir_3m': forecast_item['wind']['deg'],
        'WndMaxSpd5s_3m': forecast_item['wind'].get('gust'),
        'PresAvg_1pnt5m': forecast_item['main']['pressure'],
        'Rain_1m_Tot': forecast_item['rain']['3h'] if 'rain' in forecast_item else 0,
        'Visibility': forecast_item.get('visibility', 0),
        'Clouds': forecast_item['clouds']['all']
    }

def map_solar_forecast_data(solar_item):
    return {
        'TIMESTAMP': datetime.utcfromtimestamp(solar_item['dt']).isoformat(),
        'Solar_2m_Avg': solar_item['radiation']['ghi']  # Assuming GHI is used for solar radiation
    }

def merge_weather_and_solar_data(weather_data, solar_data):
    weather_df = pd.DataFrame(weather_data)
    solar_df = pd.DataFrame(solar_data)
    merged_df = pd.merge(weather_df, solar_df, on='TIMESTAMP', how='left')
    return merged_df

def calculate_et(df):
    df_daily = df.set_index('TIMESTAMP').resample('D').mean()
    
    required_columns = ['Ta_2m_Avg', 'TaMax_2m', 'TaMin_2m', 'RH_2m_Avg', 'WndAveSpd_3m', 'Solar_2m_Avg']
    df_daily = df_daily.dropna(subset=required_columns)
    
    if df_daily.empty:
        logger.warning("Dataframe is empty after resampling and dropping NaN values.")
        return pd.DataFrame(columns=['TIMESTAMP', 'et'])
    
    # Convert solar radiation to MJ/m^2/day
    df_daily['Solar_2m_Avg_MJ'] = df_daily['Solar_2m_Avg'] * 0.0864
    
    lat_rad = LATITUDE * np.pi / 180
    
    inputs = {
        'tmean': df_daily['Ta_2m_Avg'],
        'wind': df_daily['WndAveSpd_3m'],
        'rs': df_daily['Solar_2m_Avg_MJ'],
        'tmax': df_daily['TaMax_2m'],
        'tmin': df_daily['TaMin_2m'],
        'rh': df_daily['RH_2m_Avg'],
        'elevation': ELEVATION,
        'lat': lat_rad
    }
    
    df_daily['et'] = pyet.combination.pm_asce(**inputs)
    
    return df_daily.reset_index()[['TIMESTAMP', 'et']]

def four_day_forecast_static(lat, lon):
    try:
        logger.info("Starting static 4-day forecast function")
        
        weather_forecast_data = get_forecast(lat, lon)
        solar_forecast_data = get_solar_radiation_forecast(lat, lon)
        
        collection_time = datetime.now(pytz.UTC)
        
        mapped_weather_data = [map_forecast_data(item, collection_time) for item in weather_forecast_data['list']]
        mapped_solar_data = [map_solar_forecast_data(item) for item in solar_forecast_data['list']]
        
        logger.info(f"Processed {len(mapped_weather_data)} weather forecast entries")
        logger.info(f"Processed {len(mapped_solar_data)} solar radiation forecast entries")
        
        df_forecast = merge_weather_and_solar_data(mapped_weather_data, mapped_solar_data)
        
        # Calculate daily precipitation sum
        df_forecast['TIMESTAMP'] = pd.to_datetime(df_forecast['TIMESTAMP'])
        df_forecast.set_index('TIMESTAMP', inplace=True)
        df_daily_precip = df_forecast['Rain_1m_Tot'].resample('D').sum().reset_index()
        
        et_df = calculate_et(df_forecast)
        
        if not et_df.empty:
            df_forecast = pd.merge(df_forecast.reset_index(), et_df, on='TIMESTAMP', how='left')
            df_forecast = pd.merge(df_forecast, df_daily_precip, on='TIMESTAMP', how='left', suffixes=('', '_daily_sum'))
        
        logger.info("Static forecast data processed successfully")
        return df_forecast
    except Exception as e:
        logger.error(f"Error processing static forecast data: {str(e)}", exc_info=True)
        return pd.DataFrame()

if __name__ == "__main__":
    lat, lon = 41.089075, -100.773775
    df_result = four_day_forecast_static(lat, lon)
    print(df_result)

        </content>
    </file>
    <file>
        <name>other_scheduling.py</name>
        <path>src\other_scheduling.py</path>
        <content>
# src/other_scheduling.py

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def get_recent_values(series, n_days=3):
    end_date = pd.Timestamp.now().floor('D')
    start_date = end_date - pd.Timedelta(days=n_days)
    
    # Ensure the series index is timezone-naive
    if series.index.tz is not None:
        series.index = series.index.tz_localize(None)
    
    recent_data = series.loc[start_date:end_date]
    return recent_data[recent_data.notnull() & (recent_data != 0)], start_date, end_date

def process_treatment_two(df, plot, file_path):
    if 'cwsi' not in df.columns or 'swsi' not in df.columns:
        logger.warning(f"Missing CWSI or SWSI data for treatment two, plot {plot}. Skipping.")
        return None
    
    recommendations = []
    for date, row in df.iterrows():
        cwsi_value = row['cwsi']
        swsi_value = row['swsi']
        
        if pd.notna(cwsi_value) and pd.notna(swsi_value):
            final_value = cwsi_value * 0.4 + swsi_value * 0.6
        elif pd.notna(cwsi_value):
            final_value = cwsi_value
        elif pd.notna(swsi_value):
            final_value = swsi_value
        else:
            final_value = None
        
        recommendation = 'Irrigate' if final_value is not None and final_value > 0.5 else 'Do not irrigate'
        recommendations.append(recommendation)
    
    df['recommendation'] = recommendations
    df.to_csv(file_path)  # Overwrite the original CSV with the new column
    
    # For the summary, we'll use today's values or the last available if today's not present
    today = pd.Timestamp.now().floor('D')
    if today in df.index:
        summary_row = df.loc[today]
    else:
        summary_row = df.iloc[-1]
    
    return {
        'plot': plot,
        'treatment': 2,
        'cwsi_avg': summary_row['cwsi'],
        'swsi_avg': summary_row['swsi'],
        'final_value': final_value,
        'recommendation': summary_row['recommendation'],
        'date': summary_row.name.strftime('%Y-%m-%d')
    }

def process_treatment_three(df, plot, file_path):
    if 'cwsi' not in df.columns:
        logger.warning(f"Missing CWSI data for treatment three, plot {plot}. Skipping.")
        return None
    
    recommendations = []
    for date, row in df.iterrows():
        cwsi_value = row['cwsi']
        
        if pd.notna(cwsi_value):
            recommendation = 'Irrigate' if cwsi_value > 0.5 else 'Do not irrigate'
        else:
            recommendation = "CWSI value is null"
        recommendations.append(recommendation)
    
    df['recommendation'] = recommendations
    df.to_csv(file_path)  # Overwrite the original CSV with the new column
    
    # For the summary, we'll use today's values or the last available if today's not present
    today = pd.Timestamp.now().floor('D')
    if today in df.index:
        summary_row = df.loc[today]
    else:
        summary_row = df.iloc[-1]
    
    return {
        'plot': plot,
        'treatment': 3,
        'cwsi_avg': summary_row['cwsi'],
        'recommendation': summary_row['recommendation'],
        'date': summary_row.name.strftime('%Y-%m-%d')
    }

def process_treatment_four(df, plot, file_path):
    if 'swsi' not in df.columns:
        logger.warning(f"Missing SWSI data for treatment four, plot {plot}. Skipping.")
        return None
    
    recommendations = []
    for date, row in df.iterrows():
        swsi_value = row['swsi']
        
        if pd.notna(swsi_value):
            recommendation = 'Irrigate' if swsi_value > 0.5 else 'Do not irrigate'
        else:
            recommendation = "SWSI value is null"
        recommendations.append(recommendation)
    
    df['recommendation'] = recommendations
    df.to_csv(file_path)  # Overwrite the original CSV with the new column
    
    # For the summary, we'll use today's values or the last available if today's not present
    today = pd.Timestamp.now().floor('D')
    if today in df.index:
        summary_row = df.loc[today]
    else:
        summary_row = df.iloc[-1]
    
    return {
        'plot': plot,
        'treatment': 4,
        'swsi_avg': summary_row['swsi'],
        'recommendation': summary_row['recommendation'],
        'date': summary_row.name.strftime('%Y-%m-%d')
    }

def process_csv_file(file_path):
    logger.info(f"Processing file for irrigation recommendation: {file_path}")
    
    df = pd.read_csv(file_path, parse_dates=['TIMESTAMP'])
    df.set_index('TIMESTAMP', inplace=True)
    
    plot_number = file_path.split('_')[-2]  # Assuming the plot number is the second-to-last part of the filename
    treatment = int(file_path.split('_')[2][3])  # Assuming the treatment number is in the format 'trtX'

    result = None
    if treatment == 2:
        result = process_treatment_two(df, plot_number, file_path)
    elif treatment == 3:
        result = process_treatment_three(df, plot_number, file_path)
    elif treatment == 4:
        result = process_treatment_four(df, plot_number, file_path)
    else:
        logger.warning(f"Unsupported treatment {treatment} for plot {plot_number}. Skipping.")
    
    return result

def main(input_folder, output_folder):
    recommendations = []

    for root, _, files in os.walk(input_folder):
        for file in files:
            if file.endswith('.csv'):
                file_path = os.path.join(root, file)
                result = process_csv_file(file_path)
                if result:
                    recommendations.append(result)

    # Create recommendations DataFrame and save to CSV
    os.makedirs(output_folder, exist_ok=True)
    output_file = os.path.join(output_folder, 'recommendations.csv')
    recommendations_df = pd.DataFrame(recommendations)
    recommendations_df.to_csv(output_file, index=False)
    logger.info(f"Recommendations saved to {output_file}")

if __name__ == "__main__":
    input_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\data"
    output_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\recommendations"
    main(input_folder, output_folder)
        </content>
    </file>
    <file>
        <name>run_analysis.py</name>
        <path>src\run_analysis.py</path>
        <content>
# src/run_analysis.py

import os
from datetime import datetime
import logging
from dat_to_csv import main as dat_to_csv_main
from cwsi_th1 import main as cwsi_corn_main
from cwsi_th2_soybean import main as cwsi_soybean_main
from swsi import main as swsi_main
from et import main as et_main
from fuzz_with_visuals import main as fuzz_main
from other_scheduling import main as other_scheduling_main
from update_bigquery_tables import main as update_bigquery_main

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_dated_folder(base_path):
    current_date = datetime.now().strftime("%Y-%m-%d")
    dated_folder = os.path.join(base_path, f"analysis-{current_date}")
    os.makedirs(dated_folder, exist_ok=True)
    return dated_folder

def get_all_subfolders(base_folder):
    return [os.path.join(base_folder, d) for d in os.listdir(base_folder) if os.path.isdir(os.path.join(base_folder, d))]

def run_pipeline():
    # Define base paths
    base_output_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et"
    
    # Create dated folders
    dated_output_folder = create_dated_folder(base_output_folder)
    dated_data_folder = os.path.join(dated_output_folder, "data")
    dated_recommendations_folder = os.path.join(dated_output_folder, "recommendations")
    
    os.makedirs(dated_data_folder, exist_ok=True)
    os.makedirs(dated_recommendations_folder, exist_ok=True)
    
    # Define base folders for corn and soybean data
    corn_base_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Projects\Students\Bryan Nsoh\Data\2024_data_corn_lnr"
    soybean_base_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Projects\Students\Bryan Nsoh\Data\Soybean Lnr"
    
    # Get all subfolders
    corn_folders = get_all_subfolders(corn_base_folder)
    soybean_folders = get_all_subfolders(soybean_base_folder)
    
    sensor_mapping_path = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\sensor_mapping.yaml"
    weather_csv_path = r"C:\Users\bnsoh2\Downloads\North_Platte_3SW_Beta_1minx.csv"
    
    logger.info("Running dat_to_csv...")
    dat_to_csv_main(corn_folders, soybean_folders, sensor_mapping_path, dated_data_folder, weather_csv_path)
    
    # Run CWSI calculation for corn
    logger.info("Calculating CWSI for corn...")
    cwsi_corn_main(dated_data_folder)
    
    # Run CWSI calculation for soybean
    logger.info("Calculating CWSI for soybean...")
    cwsi_soybean_main(dated_data_folder)
    
    # Run SWSI calculation
    logger.info("Calculating SWSI...")
    swsi_main(dated_data_folder)
    
    # Run ET calculation
    logger.info("Calculating ET...")
    et_main(dated_data_folder)
    
    # Run fuzzy logic irrigation scheduling
    logger.info("Running fuzzy logic irrigation scheduling...")
    fuzz_output_file = os.path.join(dated_recommendations_folder, "fuzzy-trt-1.csv")
    fuzz_main(dated_data_folder, fuzz_output_file)
    
    # Run other scheduling methods
    logger.info("Running other scheduling methods...")
    other_scheduling_main(dated_data_folder, dated_recommendations_folder)
    
    # Update BigQuery tables
    logger.info("Updating BigQuery tables...")
    update_bigquery_main(dated_data_folder)
    
    logger.info("Analysis pipeline complete. Results saved in: " + dated_output_folder)

if __name__ == "__main__":
    run_pipeline()
        </content>
    </file>
    <file>
        <name>swsi.py</name>
        <path>src\swsi.py</path>
        <content>
import os
import pandas as pd
import numpy as np
import logging
import re
from datetime import timedelta
from collections import defaultdict

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

SOIL_DATA = {
    '8815': {'fc': 0.269, 'pwp': 0.115},
    '8816': {'fc': 0.279, 'pwp': 0.126},
    '8869': {'fc': 0.291, 'pwp': 0.143}
}
AVG_FC = np.mean([data['fc'] for data in SOIL_DATA.values()])
AVG_PWP = np.mean([data['pwp'] for data in SOIL_DATA.values()])
MAD = 0.45

def calculate_soil_properties():
    fc = AVG_FC
    pwp = AVG_PWP
    awc = fc - pwp
    vwct = fc - MAD * awc
    return fc, pwp, awc, vwct

def calculate_swsi(row, df, tdr_columns, tdr_usage_summary):
    valid_vwc = {}
    current_timestamp = row['TIMESTAMP']
    three_days_ago = current_timestamp - timedelta(days=3)
    
    for col in tdr_columns:
        if pd.notna(row[col]) and row[col] != 0:
            valid_vwc[col] = row[col] / 100
            tdr_usage_summary[col]['current'] += 1
        else:
            # Look for the last valid value within the past 3 days
            mask = (df['TIMESTAMP'] <= current_timestamp) & \
                   (df['TIMESTAMP'] > three_days_ago) & \
                   (df[col].notna()) & (df[col] != 0)
            last_valid = df.loc[mask, col].iloc[-1] if mask.any() else None
            if pd.notna(last_valid):
                valid_vwc[col] = last_valid / 100
                tdr_usage_summary[col]['past'] += 1
    
    if len(valid_vwc) < 1:
        tdr_usage_summary['missing_data'] += 1
        return None

    avg_vwc = np.mean(list(valid_vwc.values()))
    fc, pwp, awc, vwct = calculate_soil_properties()
    
    if avg_vwc < vwct:
        return (vwct - avg_vwc) / (vwct - pwp)
    else:
        return 0

def select_tdr_columns(df):
    pattern = r'^TDR\d{4}[A-E][1256](06|18|30)24$'
    tdr_columns = [col for col in df.columns if re.match(pattern, col)]
    logger.info(f"Selected TDR columns: {tdr_columns}")
    return tdr_columns

def process_csv_file(file_path):
    logger.info(f"Processing file: {file_path}")
    
    df = pd.read_csv(file_path, parse_dates=['TIMESTAMP'])
    
    tdr_columns = select_tdr_columns(df)
    if not tdr_columns:
        logger.warning(f"No valid TDR columns found in {file_path}")
        return None
    
    tdr_usage_summary = {col: {'current': 0, 'past': 0} for col in tdr_columns}
    tdr_usage_summary['missing_data'] = 0

    df['all_tdr_invalid'] = (df[tdr_columns] == 0).all(axis=1) | df[tdr_columns].isnull().all(axis=1)
    df_filtered = df[~df['all_tdr_invalid']].copy()
    
    df_filtered['swsi'] = df_filtered.apply(lambda row: calculate_swsi(row, df_filtered, tdr_columns, tdr_usage_summary), axis=1)
    df.loc[df_filtered.index, 'swsi'] = df_filtered['swsi']
    df = df.drop(columns=['all_tdr_invalid'])
    
    non_zero_tdr = df_filtered[tdr_columns].values.flatten()
    non_zero_tdr = non_zero_tdr[~np.isnan(non_zero_tdr) & (non_zero_tdr != 0)]
    
    valid_swsi = df_filtered['swsi'].dropna()
    
    logger.info("\nTDR Usage Summary:")
    for col in tdr_columns:
        current = tdr_usage_summary[col]['current']
        past = tdr_usage_summary[col]['past']
        total = current + past
        if total > 0:
            logger.info(f"{col}: Used {total} times ({current} current, {past} past 3 days)")
    logger.info(f"Missing data points: {tdr_usage_summary['missing_data']}")

    if len(non_zero_tdr) > 0:
        logger.info(f"\nTDR stats - Count: {len(non_zero_tdr)}, Mean: {np.mean(non_zero_tdr):.4f}, Min: {np.min(non_zero_tdr):.4f}, Max: {np.max(non_zero_tdr):.4f}")
    else:
        logger.info("\nNo valid TDR values found")
    
    if len(valid_swsi) > 0:
        logger.info(f"\nSWSI stats - Count: {len(valid_swsi)}, Mean: {valid_swsi.mean():.4f}, Min: {valid_swsi.min():.4f}, Max: {valid_swsi.max():.4f}")
    else:
        logger.info("\nNo valid SWSI values calculated")
    
    df.to_csv(file_path, index=False)
    logger.info(f"\nUpdated SWSI values in file: {file_path}")
    
    return df

def main(input_folder):
    for root, _, files in os.walk(input_folder):
        for file in files:
            if file.endswith('.csv'):
                file_path = os.path.join(root, file)
                process_csv_file(file_path)

if __name__ == "__main__":
    input_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\data"
    main(input_folder)
        </content>
    </file>
    <file>
        <name>test.py</name>
        <path>src\test.py</path>
        <content>
import os
import sys
import requests
import json
from dotenv import load_dotenv, find_dotenv

def get_api_key():
    # Try to find and load the .env file
    dotenv_path = find_dotenv()
    if not dotenv_path:
        raise FileNotFoundError(".env file not found. Please create one with your API key.")
    
    load_dotenv(dotenv_path, override=True)
    api_key = os.getenv('OPENWEATHERMAP_API_KEY')
    
    if not api_key:
        raise ValueError("API key not found in .env file. Please set OPENWEATHERMAP_API_KEY.")
    
    return api_key

def test_solar_radiation_api(lat, lon, api_key):
    solar_url = "https://api.openweathermap.org/data/2.5/solar_radiation/forecast"
    params = {
        'lat': lat,
        'lon': lon,
        'appid': api_key
    }
    try:
        response = requests.get(solar_url, params=params, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.HTTPError as http_err:
        if response.status_code == 401:
            print(f"Error 401: Unauthorized. Please check your API key.")
        elif response.status_code == 404:
            print(f"Error 404: API endpoint not found. Please check the URL.")
        else:
            print(f"HTTP error occurred: {http_err}")
    except requests.exceptions.ConnectionError:
        print("Error: Unable to connect to the API. Please check your internet connection.")
    except requests.exceptions.Timeout:
        print("Error: API request timed out. Please try again later.")
    except requests.exceptions.RequestException as err:
        print(f"An error occurred: {err}")
    return None

def display_solar_data(data):
    if not data:
        return
    
    print("\nSolar Radiation Forecast Data:")
    print(json.dumps(data, indent=2))
    
    if 'list' in data:
        print("\nSummary:")
        for item in data['list'][:5]:  # Display first 5 entries
            dt = item.get('dt', 'N/A')
            ghi = item.get('radiation', {}).get('ghi', 'N/A')
            print(f"Timestamp: {dt}, Global Horizontal Irradiance: {ghi} W/m")

def main():
    try:
        api_key = get_api_key()
        print(f"API Key loaded successfully: {api_key[:5]}...{api_key[-5:]}")
        
        lat, lon = 41.089075, -100.773775
        print(f"\nTesting Solar Radiation API for coordinates: {lat}, {lon}")
        
        solar_data = test_solar_radiation_api(lat, lon, api_key)
        if solar_data:
            display_solar_data(solar_data)
        else:
            print("Failed to retrieve solar radiation data.")
    
    except FileNotFoundError as e:
        print(f"Error: {e}")
    except ValueError as e:
        print(f"Error: {e}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

if __name__ == "__main__":
    main()
        </content>
    </file>
    <file>
        <name>update_bigquery_tables.py</name>
        <path>src\update_bigquery_tables.py</path>
        <content>
# src/update_bigquery_tables.py

import os
import pandas as pd
import re
from google.cloud import bigquery
from google.oauth2 import service_account
from dotenv import load_dotenv
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def create_bigquery_client():
    load_dotenv()
    credentials_path = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
    
    if not credentials_path:
        logger.error("Error: GOOGLE_APPLICATION_CREDENTIALS not set in .env file")
        raise ValueError("GOOGLE_APPLICATION_CREDENTIALS not set in .env file")
    
    try:
        credentials = service_account.Credentials.from_service_account_file(
            credentials_path,
            scopes=["https://www.googleapis.com/auth/cloud-platform"],
        )
        return bigquery.Client(credentials=credentials, project=credentials.project_id)
    except Exception as e:
        logger.error(f"Error: Error creating BigQuery client: {e}")
        raise

def extract_dataset_and_table(file_name):
    match = re.match(r'(LINEAR_\w+_trt\d+)_plot_(\d+)_\d+\.csv', file_name)
    if match:
        dataset_name, table_name = match.groups()
        return dataset_name, f"plot_{table_name}"
    else:
        raise ValueError(f"Unable to extract dataset and table name from filename: {file_name}")

def upload_to_bigquery(df, client, dataset_id, table_id):
    full_table_id = f"{client.project}.{dataset_id}.{table_id}"
    
    job_config = bigquery.LoadJobConfig(
        autodetect=True,
        write_disposition="WRITE_TRUNCATE",
    )

    try:
        job = client.load_table_from_dataframe(df, full_table_id, job_config=job_config)
        job.result()  # Wait for the job to complete
        logger.info(f"Loaded {len(df)} rows into {full_table_id}")
    except Exception as e:
        logger.error(f"Failed to upload data to {full_table_id}: {str(e)}")

def process_and_upload_file(file_path, client):
    file_name = os.path.basename(file_path)
    dataset_id, table_id = extract_dataset_and_table(file_name)
    
    df = pd.read_csv(file_path, parse_dates=['TIMESTAMP'])
    
    upload_to_bigquery(df, client, dataset_id, table_id)

def main(input_folder):
    client = create_bigquery_client()
    
    for root, _, files in os.walk(input_folder):
        for file in files:
            if file.endswith('.csv'):
                file_path = os.path.join(root, file)
                logger.info(f"Processing file: {file_path}")
                process_and_upload_file(file_path, client)

if __name__ == "__main__":
    input_folder = r"C:\Users\bnsoh2\OneDrive - University of Nebraska-Lincoln\Documents\Projects\masters-project\cwsi-swsi-et\data"
    main(input_folder)
        </content>
    </file>
</directory>
</repository_structure>
